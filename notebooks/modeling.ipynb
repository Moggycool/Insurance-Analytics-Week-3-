{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fdcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Insurance Claim Severity Modeling - EDA & Results Dashboard\n",
    "# \n",
    "# This notebook provides a comprehensive overview of the insurance claim severity modeling pipeline, including data exploration, model results, and insights.\n",
    "# \n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Configuration\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up paths\n",
    "BASE_PATH = Path.cwd().parent\n",
    "DATA_PATH = BASE_PATH / \"data\"\n",
    "MODELS_PATH = BASE_PATH / \"models\"\n",
    "RESULTS_PATH = BASE_PATH / \"Results\"\n",
    "NOTEBOOKS_PATH = BASE_PATH / \"notebooks\"\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(BASE_PATH / \"src\"))\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Base path: {BASE_PATH}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Load Data\n",
    "\n",
    "# %%\n",
    "def load_data():\n",
    "    \"\"\"Load all available datasets\"\"\"\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Load processed data\n",
    "    try:\n",
    "        processed_path = DATA_PATH / \"processed\"\n",
    "        files = list(processed_path.glob(\"*.csv\"))\n",
    "        \n",
    "        for file in files:\n",
    "            name = file.stem\n",
    "            print(f\"Loading {name}...\")\n",
    "            try:\n",
    "                data_dict[name] = pd.read_csv(file)\n",
    "                print(f\"  Shape: {data_dict[name].shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {file}: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading processed data: {e}\")\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading datasets...\")\n",
    "data_dict = load_data()\n",
    "\n",
    "# Display available datasets\n",
    "print(\"\\nAvailable datasets:\")\n",
    "for name, df in data_dict.items():\n",
    "    print(f\"  - {name}: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91cfe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Data Exploration\n",
    "\n",
    "# %%\n",
    "if 'claim_policies' in data_dict:\n",
    "    df = data_dict['claim_policies'].copy()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"DATA EXPLORATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nüìä Dataset Shape: {df.shape}\")\n",
    "    print(f\"üìã Columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nData Types:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing %': missing_pct\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        display(missing_df)\n",
    "    else:\n",
    "        print(\"  No missing values found!\")\n",
    "    \n",
    "    # Target variable analysis\n",
    "    if 'TotalClaims' in df.columns:\n",
    "        print(\"\\nüéØ Target Variable Analysis (TotalClaims):\")\n",
    "        target_stats = df['TotalClaims'].describe()\n",
    "        display(pd.DataFrame(target_stats).T)\n",
    "        \n",
    "        # Plot target distribution\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Histogram\n",
    "        axes[0].hist(df['TotalClaims'], bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[0].axvline(df['TotalClaims'].mean(), color='red', linestyle='--', \n",
    "                       label=f'Mean: R{df[\"TotalClaims\"].mean():,.2f}')\n",
    "        axes[0].axvline(df['TotalClaims'].median(), color='green', linestyle='--',\n",
    "                       label=f'Median: R{df[\"TotalClaims\"].median():,.2f}')\n",
    "        axes[0].set_xlabel('Claim Amount (R)')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        axes[0].set_title('Distribution of Claim Amounts')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Log transformation\n",
    "        if (df['TotalClaims'] > 0).all():\n",
    "            log_claims = np.log1p(df['TotalClaims'])\n",
    "            axes[1].hist(log_claims, bins=50, edgecolor='black', alpha=0.7)\n",
    "            axes[1].set_xlabel('Log(1 + Claim Amount)')\n",
    "            axes[1].set_ylabel('Frequency')\n",
    "            axes[1].set_title('Log-Transformed Claim Amounts')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'Log transform not possible\\n(negative values present)',\n",
    "                        ha='center', va='center', transform=axes[1].transAxes)\n",
    "            axes[1].set_title('Log-Transformed Claim Amounts')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Top claims\n",
    "        print(\"\\nüí∞ Top 10 Largest Claims:\")\n",
    "        top_claims = df.nlargest(10, 'TotalClaims')[['PolicyID', 'TotalClaims']].copy()\n",
    "        top_claims['TotalClaims'] = top_claims['TotalClaims'].apply(lambda x: f'R{x:,.2f}')\n",
    "        display(top_claims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884977d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Load Model Results\n",
    "\n",
    "# %%\n",
    "def load_model_results():\n",
    "    \"\"\"Load model evaluation results\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Load model comparison\n",
    "    model_comp_path = MODELS_PATH / \"model_comparison.json\"\n",
    "    if model_comp_path.exists():\n",
    "        try:\n",
    "            with open(model_comp_path, 'r') as f:\n",
    "                model_data = json.load(f)\n",
    "            \n",
    "            # Check the structure of model_data\n",
    "            print(f\"Model comparison data type: {type(model_data)}\")\n",
    "            \n",
    "            if isinstance(model_data, dict):\n",
    "                print(f\"Model comparison keys: {list(model_data.keys())}\")\n",
    "                \n",
    "                # Handle the specific structure you have\n",
    "                if 'model_comparison' in model_data:\n",
    "                    print(\"Found 'model_comparison' key - extracting nested data\")\n",
    "                    results['model_comparison'] = model_data['model_comparison']\n",
    "                    \n",
    "                    if 'detailed_metrics' in model_data:\n",
    "                        results['detailed_metrics'] = model_data['detailed_metrics']\n",
    "                    \n",
    "                    if 'best_model' in model_data:\n",
    "                        results['best_model'] = model_data['best_model']\n",
    "                else:\n",
    "                    # It's already in the right format\n",
    "                    results['model_comparison'] = model_data\n",
    "                \n",
    "            print(f\"‚úì Loaded model comparison results\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model comparison: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Load cross-validation results\n",
    "    cv_path = MODELS_PATH / \"cross_validation_results.json\"\n",
    "    if cv_path.exists():\n",
    "        with open(cv_path, 'r') as f:\n",
    "            results['cv_results'] = json.load(f)\n",
    "        print(f\"‚úì Loaded cross-validation results\")\n",
    "    \n",
    "    # Load Lasso best params\n",
    "    lasso_params_path = MODELS_PATH / \"Lasso_best_params.json\"\n",
    "    if lasso_params_path.exists():\n",
    "        with open(lasso_params_path, 'r') as f:\n",
    "            results['lasso_params'] = json.load(f)\n",
    "        print(f\"‚úì Loaded Lasso parameters\")\n",
    "    \n",
    "    # Load Linear Regression best params\n",
    "    lr_params_path = MODELS_PATH / \"LinearRegression_best_params.json\"\n",
    "    if lr_params_path.exists():\n",
    "        with open(lr_params_path, 'r') as f:\n",
    "            results['lr_params'] = json.load(f)\n",
    "        print(f\"‚úì Loaded Linear Regression parameters\")\n",
    "    \n",
    "    # Load task 4 reports\n",
    "    task4_json_path = RESULTS_PATH / \"Task4_Reports\" / \"task4_comprehensive_report.json\"\n",
    "    if task4_json_path.exists():\n",
    "        with open(task4_json_path, 'r') as f:\n",
    "            results['task4_report'] = json.load(f)\n",
    "        print(f\"‚úì Loaded Task 4 comprehensive report\")\n",
    "    \n",
    "    task4_md_path = RESULTS_PATH / \"Task4_Reports\" / \"task4_final_report.md\"\n",
    "    if task4_md_path.exists():\n",
    "        with open(task4_md_path, 'r') as f:\n",
    "            results['task4_md'] = f.read()\n",
    "        print(f\"‚úì Loaded Task 4 markdown report\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Loading model results...\")\n",
    "model_results = load_model_results()\n",
    "\n",
    "# Display what we loaded\n",
    "print(\"\\nüìä Loaded model results:\")\n",
    "for key, value in model_results.items():\n",
    "    if key != 'task4_md':  # Don't print large markdown content\n",
    "        print(f\"  {key}: {type(value)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cffbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Model Performance Analysis\n",
    "\n",
    "# %%\n",
    "if 'model_comparison' in model_results:\n",
    "    print(\"=\"*60)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model_data = model_results['model_comparison']\n",
    "    print(f\"Model comparison data type: {type(model_data)}\")\n",
    "    \n",
    "    if isinstance(model_data, dict):\n",
    "        print(f\"Number of models: {len(model_data)}\")\n",
    "        print(f\"Model names: {list(model_data.keys())}\")\n",
    "        \n",
    "        # Let's see the structure of the first model's data\n",
    "        if model_data:\n",
    "            first_model = list(model_data.keys())[0]\n",
    "            print(f\"\\nFirst model '{first_model}' data structure:\")\n",
    "            print(f\"  Type: {type(model_data[first_model])}\")\n",
    "            if isinstance(model_data[first_model], dict):\n",
    "                print(f\"  Keys: {list(model_data[first_model].keys())}\")\n",
    "                print(f\"  Sample values:\")\n",
    "                for key, value in list(model_data[first_model].items())[:5]:\n",
    "                    print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # Extract metrics from the nested structure\n",
    "    print(\"\\nüìà Extracting model metrics...\")\n",
    "    \n",
    "    # Create a list to store model metrics\n",
    "    model_metrics = []\n",
    "    \n",
    "    if isinstance(model_data, dict):\n",
    "        for model_name, metrics in model_data.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                # Extract relevant metrics\n",
    "                model_info = {\n",
    "                    'Model': model_name,\n",
    "                    'R2': metrics.get('r2', metrics.get('R2', np.nan)),\n",
    "                    'MAE': metrics.get('mae', metrics.get('MAE', np.nan)),\n",
    "                    'RMSE': metrics.get('rmse', metrics.get('RMSE', np.nan)),\n",
    "                    'Training_Time': metrics.get('training_time', metrics.get('Training_Time', np.nan))\n",
    "                }\n",
    "                model_metrics.append(model_info)\n",
    "    \n",
    "    if model_metrics:\n",
    "        # Create DataFrame\n",
    "        comp_df = pd.DataFrame(model_metrics)\n",
    "        \n",
    "        # Set Model as index\n",
    "        comp_df.set_index('Model', inplace=True)\n",
    "        \n",
    "        # Sort by R¬≤\n",
    "        if 'R2' in comp_df.columns:\n",
    "            comp_df = comp_df.sort_values('R2', ascending=False)\n",
    "        \n",
    "        print(f\"\\nModel comparison DataFrame shape: {comp_df.shape}\")\n",
    "        print(f\"Columns: {comp_df.columns.tolist()}\")\n",
    "        \n",
    "        print(\"\\nüìä Model Performance Metrics:\")\n",
    "        display(comp_df)\n",
    "        \n",
    "        # Create visualization\n",
    "        num_models = len(comp_df)\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # R¬≤ Score\n",
    "        if 'R2' in comp_df.columns:\n",
    "            axes[0, 0].barh(comp_df.index, comp_df['R2'], color='skyblue')\n",
    "            axes[0, 0].set_xlabel('R¬≤ Score')\n",
    "            axes[0, 0].set_title('Model R¬≤ Scores')\n",
    "            axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, (idx, row) in enumerate(comp_df.iterrows()):\n",
    "                axes[0, 0].text(row['R2'] + 0.01, i, f\"{row['R2']:.3f}\", \n",
    "                              va='center', fontsize=10)\n",
    "        else:\n",
    "            axes[0, 0].text(0.5, 0.5, 'R¬≤ data not available',\n",
    "                          ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "            axes[0, 0].set_title('R¬≤ Scores (Not Available)')\n",
    "        \n",
    "        # MAE\n",
    "        if 'MAE' in comp_df.columns:\n",
    "            axes[0, 1].barh(comp_df.index, comp_df['MAE'], color='lightcoral')\n",
    "            axes[0, 1].set_xlabel('MAE (R)')\n",
    "            axes[0, 1].set_title('Mean Absolute Error')\n",
    "            axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, (idx, row) in enumerate(comp_df.iterrows()):\n",
    "                axes[0, 1].text(row['MAE'] + comp_df['MAE'].max()*0.01, i, \n",
    "                              f\"R{row['MAE']:,.2f}\", va='center', fontsize=10)\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'MAE data not available',\n",
    "                          ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "            axes[0, 1].set_title('MAE (Not Available)')\n",
    "        \n",
    "        # RMSE\n",
    "        if 'RMSE' in comp_df.columns:\n",
    "            axes[1, 0].barh(comp_df.index, comp_df['RMSE'], color='lightgreen')\n",
    "            axes[1, 0].set_xlabel('RMSE (R)')\n",
    "            axes[1, 0].set_title('Root Mean Squared Error')\n",
    "            axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, (idx, row) in enumerate(comp_df.iterrows()):\n",
    "                axes[1, 0].text(row['RMSE'] + comp_df['RMSE'].max()*0.01, i, \n",
    "                              f\"R{row['RMSE']:,.2f}\", va='center', fontsize=10)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'RMSE data not available',\n",
    "                          ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].set_title('RMSE (Not Available)')\n",
    "        \n",
    "        # Training Time\n",
    "        if 'Training_Time' in comp_df.columns:\n",
    "            axes[1, 1].barh(comp_df.index, comp_df['Training_Time'], color='gold')\n",
    "            axes[1, 1].set_xlabel('Training Time (seconds)')\n",
    "            axes[1, 1].set_title('Model Training Time')\n",
    "            axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, (idx, row) in enumerate(comp_df.iterrows()):\n",
    "                axes[1, 1].text(row['Training_Time'] + comp_df['Training_Time'].max()*0.01, i, \n",
    "                              f\"{row['Training_Time']:.2f}s\", va='center', fontsize=10)\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'Training time data not available',\n",
    "                          ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "            axes[1, 1].set_title('Training Time (Not Available)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Best model\n",
    "        if 'R2' in comp_df.columns and not comp_df['R2'].isna().all():\n",
    "            best_model = comp_df['R2'].idxmax()\n",
    "            best_r2 = comp_df.loc[best_model, 'R2']\n",
    "            print(f\"\\nüèÜ Best Performing Model: {best_model}\")\n",
    "            print(f\"   R¬≤ Score: {best_r2:.4f}\")\n",
    "            \n",
    "            if 'MAE' in comp_df.columns and not pd.isna(comp_df.loc[best_model, 'MAE']):\n",
    "                print(f\"   MAE: R{comp_df.loc[best_model, 'MAE']:,.2f}\")\n",
    "            \n",
    "            if 'RMSE' in comp_df.columns and not pd.isna(comp_df.loc[best_model, 'RMSE']):\n",
    "                print(f\"   RMSE: R{comp_df.loc[best_model, 'RMSE']:,.2f}\")\n",
    "            \n",
    "            if 'Training_Time' in comp_df.columns and not pd.isna(comp_df.loc[best_model, 'Training_Time']):\n",
    "                print(f\"   Training Time: {comp_df.loc[best_model, 'Training_Time']:.2f}s\")\n",
    "            \n",
    "            # Also check if we have a 'best_model' key\n",
    "            if 'best_model' in model_results:\n",
    "                print(f\"\\nüìå Designated Best Model from results: {model_results['best_model']}\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è Could not determine best model (R¬≤ scores not available)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No model metrics found in the data\")\n",
    "\n",
    "# Check for detailed metrics\n",
    "if 'detailed_metrics' in model_results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED MODEL METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    detailed = model_results['detailed_metrics']\n",
    "    if isinstance(detailed, dict):\n",
    "        print(\"Detailed metrics available for models:\")\n",
    "        for model_name, metrics in detailed.items():\n",
    "            print(f\"\\n  {model_name}:\")\n",
    "            if isinstance(metrics, dict):\n",
    "                for key, value in list(metrics.items())[:5]:  # Show first 5\n",
    "                    print(f\"    {key}: {value}\")\n",
    "                if len(metrics) > 5:\n",
    "                    print(f\"    ... and {len(metrics) - 5} more metrics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da39063",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Cross-Validation Results\n",
    "\n",
    "# %%\n",
    "if 'cv_results' in model_results:\n",
    "    print(\"=\"*60)\n",
    "    print(\"CROSS-VALIDATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    cv_data = model_results['cv_results']\n",
    "    \n",
    "    # Check structure\n",
    "    print(f\"CV data type: {type(cv_data)}\")\n",
    "    \n",
    "    if isinstance(cv_data, dict):\n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # R¬≤ scores across folds\n",
    "        plotted_r2 = False\n",
    "        for model, scores in cv_data.items():\n",
    "            if isinstance(scores, dict) and 'test_r2' in scores:\n",
    "                if isinstance(scores['test_r2'], list):\n",
    "                    axes[0].plot(range(1, len(scores['test_r2']) + 1), \n",
    "                                scores['test_r2'], \n",
    "                                marker='o', \n",
    "                                label=model,\n",
    "                                linewidth=2)\n",
    "                    plotted_r2 = True\n",
    "        \n",
    "        if plotted_r2:\n",
    "            axes[0].set_xlabel('Fold Number')\n",
    "            axes[0].set_ylabel('R¬≤ Score')\n",
    "            axes[0].set_title('Cross-Validation R¬≤ Scores by Fold')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0].text(0.5, 0.5, 'R¬≤ scores not available',\n",
    "                       ha='center', va='center', transform=axes[0].transAxes)\n",
    "            axes[0].set_title('R¬≤ Scores (Not Available)')\n",
    "        \n",
    "        # RMSE scores across folds\n",
    "        plotted_rmse = False\n",
    "        for model, scores in cv_data.items():\n",
    "            if isinstance(scores, dict) and 'test_rmse' in scores:\n",
    "                if isinstance(scores['test_rmse'], list):\n",
    "                    axes[1].plot(range(1, len(scores['test_rmse']) + 1), \n",
    "                                scores['test_rmse'], \n",
    "                                marker='s', \n",
    "                                label=model,\n",
    "                                linewidth=2)\n",
    "                    plotted_rmse = True\n",
    "        \n",
    "        if plotted_rmse:\n",
    "            axes[1].set_xlabel('Fold Number')\n",
    "            axes[1].set_ylabel('RMSE (R)')\n",
    "            axes[1].set_title('Cross-Validation RMSE by Fold')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'RMSE scores not available',\n",
    "                       ha='center', va='center', transform=axes[1].transAxes)\n",
    "            axes[1].set_title('RMSE Scores (Not Available)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\nüìä Cross-Validation Summary Statistics:\")\n",
    "        summary_data = []\n",
    "        \n",
    "        for model, scores in cv_data.items():\n",
    "            if isinstance(scores, dict) and 'test_r2' in scores:\n",
    "                if isinstance(scores['test_r2'], list) and scores['test_r2']:\n",
    "                    r2_mean = np.mean(scores['test_r2'])\n",
    "                    r2_std = np.std(scores['test_r2'])\n",
    "                    summary_data.append({\n",
    "                        'Model': model,\n",
    "                        'Mean R¬≤': f\"{r2_mean:.4f}\",\n",
    "                        'Std R¬≤': f\"{r2_std:.4f}\",\n",
    "                        'R¬≤ Range': f\"{min(scores['test_r2']):.4f} - {max(scores['test_r2']):.4f}\"\n",
    "                    })\n",
    "        \n",
    "        if summary_data:\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            display(summary_df)\n",
    "        else:\n",
    "            print(\"No R¬≤ score data available for summary\")\n",
    "    else:\n",
    "        print(\"CV data is not in expected dictionary format\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Hyperparameter Tuning Results\n",
    "\n",
    "# %%\n",
    "print(\"=\"*60)\n",
    "print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'lasso_params' in model_results:\n",
    "    print(\"\\nüîß Lasso Regression - Best Parameters:\")\n",
    "    lasso_params = model_results['lasso_params']\n",
    "    if isinstance(lasso_params, dict):\n",
    "        for param, value in lasso_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "    else:\n",
    "        print(f\"  Parameters (raw): {lasso_params}\")\n",
    "\n",
    "if 'lr_params' in model_results:\n",
    "    print(\"\\nüîß Linear Regression - Best Parameters:\")\n",
    "    lr_params = model_results['lr_params']\n",
    "    if isinstance(lr_params, dict):\n",
    "        for param, value in lr_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "    else:\n",
    "        print(f\"  Parameters (raw): {lr_params}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Feature Importance Analysis\n",
    "\n",
    "# %%\n",
    "def analyze_feature_importance():\n",
    "    \"\"\"Analyze feature importance from models\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Try to load tree-based models for feature importance\n",
    "    tree_models = ['RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM', 'DecisionTree']\n",
    "    \n",
    "    for model_name in tree_models:\n",
    "        model_path = MODELS_PATH / f\"{model_name}.pkl\"\n",
    "        if model_path.exists():\n",
    "            try:\n",
    "                with open(model_path, 'rb') as f:\n",
    "                    model = pickle.load(f)\n",
    "                \n",
    "                if hasattr(model, 'feature_importances_'):\n",
    "                    print(f\"\\nüå≥ {model_name} Feature Importance (Top 10):\")\n",
    "                    \n",
    "                    # Try to load feature names\n",
    "                    try:\n",
    "                        # Load preprocessor to get feature names\n",
    "                        preprocessor_path = MODELS_PATH / \"preprocessor.pkl\"\n",
    "                        if preprocessor_path.exists():\n",
    "                            with open(preprocessor_path, 'rb') as f:\n",
    "                                preprocessor = pickle.load(f)\n",
    "                            \n",
    "                            # Get feature names from preprocessor\n",
    "                            if hasattr(preprocessor, 'get_feature_names_out'):\n",
    "                                feature_names = preprocessor.get_feature_names_out()\n",
    "                            else:\n",
    "                                # Try to get from training data\n",
    "                                train_path = DATA_PATH / \"processed\" / \"train_data.csv\"\n",
    "                                if train_path.exists():\n",
    "                                    train_df = pd.read_csv(train_path)\n",
    "                                    feature_names = [col for col in train_df.columns \n",
    "                                                   if col not in ['Log_TotalClaims', 'TotalClaims', 'HighClaim']]\n",
    "                                else:\n",
    "                                    feature_names = [f\"feature_{i}\" for i in range(len(model.feature_importances_))]\n",
    "                        else:\n",
    "                            feature_names = [f\"feature_{i}\" for i in range(len(model.feature_importances_))]\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"  Warning: Could not get feature names: {e}\")\n",
    "                        feature_names = [f\"feature_{i}\" for i in range(len(model.feature_importances_))]\n",
    "                    \n",
    "                    # Create importance DataFrame\n",
    "                    importance_df = pd.DataFrame({\n",
    "                        'feature': feature_names[:len(model.feature_importances_)],\n",
    "                        'importance': model.feature_importances_\n",
    "                    }).sort_values('importance', ascending=False)\n",
    "                    \n",
    "                    # Display top 10\n",
    "                    display(importance_df.head(10))\n",
    "                    \n",
    "                    # Plot\n",
    "                    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                    top_10 = importance_df.head(10).sort_values('importance')\n",
    "                    ax.barh(top_10['feature'], top_10['importance'], color='teal')\n",
    "                    ax.set_xlabel('Importance Score')\n",
    "                    ax.set_title(f'{model_name} - Top 10 Feature Importances')\n",
    "                    ax.grid(True, alpha=0.3, axis='x')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {model_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"  {model_name}.pkl not found\")\n",
    "\n",
    "# Run feature importance analysis\n",
    "analyze_feature_importance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Task 4 Insights\n",
    "\n",
    "# %%\n",
    "if 'task4_report' in model_results:\n",
    "    print(\"=\"*60)\n",
    "    print(\"TASK 4: COMPREHENSIVE INSIGHTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    report = model_results['task4_report']\n",
    "    \n",
    "    # Display key insights\n",
    "    if 'key_insights' in report:\n",
    "        print(\"\\nüîç Key Insights:\")\n",
    "        for i, insight in enumerate(report['key_insights'], 1):\n",
    "            print(f\"  {i}. {insight}\")\n",
    "    elif 'insights' in report:\n",
    "        print(\"\\nüîç Key Insights:\")\n",
    "        if isinstance(report['insights'], list):\n",
    "            for i, insight in enumerate(report['insights'], 1):\n",
    "                print(f\"  {i}. {insight}\")\n",
    "        else:\n",
    "            print(f\"  {report['insights']}\")\n",
    "    \n",
    "    if 'recommendations' in report:\n",
    "        print(\"\\nüí° Business Recommendations:\")\n",
    "        for i, rec in enumerate(report['recommendations'], 1):\n",
    "            print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    if 'limitations' in report:\n",
    "        print(\"\\n‚ö†Ô∏è Limitations:\")\n",
    "        for i, limit in enumerate(report['limitations'], 1):\n",
    "            print(f\"  {i}. {limit}\")\n",
    "    \n",
    "    # Display technical metrics\n",
    "    if 'technical_summary' in report:\n",
    "        print(\"\\nüìä Technical Summary:\")\n",
    "        tech_summary = report['technical_summary']\n",
    "        if isinstance(tech_summary, dict):\n",
    "            for key, value in tech_summary.items():\n",
    "                if isinstance(value, dict):\n",
    "                    print(f\"\\n  {key}:\")\n",
    "                    for sub_key, sub_value in value.items():\n",
    "                        print(f\"    {sub_key}: {sub_value}\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"  {tech_summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c41a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Model Comparison Visualizations\n",
    "\n",
    "# %%\n",
    "# Load model comparison images if they exist\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for comparison images\n",
    "comparison_images = {\n",
    "    'combined': MODELS_PATH / \"model_comparison_combined.png\",\n",
    "    'r2': MODELS_PATH / \"model_comparison_r2.png\"\n",
    "}\n",
    "\n",
    "for img_name, img_path in comparison_images.items():\n",
    "    if img_path.exists():\n",
    "        print(f\"\\nüìä Displaying {img_name} comparison:\")\n",
    "        try:\n",
    "            img = plt.imread(img_path)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Model Comparison - {img_name.upper()}')\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying {img_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ÑπÔ∏è {img_name} image not found at {img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 11. Prediction Analysis\n",
    "\n",
    "# %%\n",
    "def analyze_predictions():\n",
    "    \"\"\"Analyze model predictions vs actual values\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"PREDICTION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Try to load test data\n",
    "    test_path = DATA_PATH / \"processed\" / \"test_data.csv\"\n",
    "    if test_path.exists():\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        \n",
    "        # Check for target columns\n",
    "        target_columns = ['Log_TotalClaims', 'TotalClaims']\n",
    "        available_targets = [col for col in target_columns if col in test_df.columns]\n",
    "        \n",
    "        if available_targets:\n",
    "            # Use the first available target\n",
    "            target_col = available_targets[0]\n",
    "            y_true = test_df[target_col]\n",
    "            \n",
    "            print(f\"Using target variable: {target_col}\")\n",
    "            \n",
    "            # Load best model\n",
    "            best_model_name = None\n",
    "            if 'model_comparison' in model_results:\n",
    "                try:\n",
    "                    model_data = model_results['model_comparison']\n",
    "                    if isinstance(model_data, dict):\n",
    "                        # Find best model by R¬≤\n",
    "                        best_r2 = -np.inf\n",
    "                        for model, metrics in model_data.items():\n",
    "                            if isinstance(metrics, dict):\n",
    "                                r2 = metrics.get('r2', metrics.get('R2', -np.inf))\n",
    "                                if r2 > best_r2:\n",
    "                                    best_r2 = r2\n",
    "                                    best_model_name = model\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Also check the 'best_model' key\n",
    "            if not best_model_name and 'best_model' in model_results:\n",
    "                best_model_name = model_results['best_model']\n",
    "            \n",
    "            if best_model_name:\n",
    "                model_path = MODELS_PATH / f\"{best_model_name}.pkl\"\n",
    "                if model_path.exists():\n",
    "                    try:\n",
    "                        with open(model_path, 'rb') as f:\n",
    "                            best_model = pickle.load(f)\n",
    "                        \n",
    "                        # Load preprocessor\n",
    "                        preprocessor_path = MODELS_PATH / \"preprocessor.pkl\"\n",
    "                        if preprocessor_path.exists():\n",
    "                            with open(preprocessor_path, 'rb') as f:\n",
    "                                preprocessor = pickle.load(f)\n",
    "                            \n",
    "                            # Prepare features\n",
    "                            X_test = test_df.drop(columns=target_columns + ['HighClaim'], \n",
    "                                                errors='ignore')\n",
    "                            \n",
    "                            # Transform features\n",
    "                            X_test_transformed = preprocessor.transform(X_test)\n",
    "                            \n",
    "                            # Make predictions\n",
    "                            y_pred = best_model.predict(X_test_transformed)\n",
    "                            \n",
    "                            # Create prediction analysis\n",
    "                            pred_df = pd.DataFrame({\n",
    "                                'Actual': y_true,\n",
    "                                'Predicted': y_pred,\n",
    "                                'Residual': y_true - y_pred,\n",
    "                                'Absolute_Error': np.abs(y_true - y_pred)\n",
    "                            })\n",
    "                            \n",
    "                            # Calculate metrics\n",
    "                            mae = mean_absolute_error(y_true, y_pred)\n",
    "                            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "                            r2 = r2_score(y_true, y_pred)\n",
    "                            \n",
    "                            print(f\"\\nüìà Prediction Metrics for {best_model_name}:\")\n",
    "                            print(f\"  R¬≤: {r2:.4f}\")\n",
    "                            print(f\"  MAE: {mae:.4f}\")\n",
    "                            print(f\"  RMSE: {rmse:.4f}\")\n",
    "                            \n",
    "                            # Plot predictions vs actual\n",
    "                            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "                            \n",
    "                            # Scatter plot\n",
    "                            axes[0, 0].scatter(y_true, y_pred, alpha=0.5, color='blue')\n",
    "                            axes[0, 0].plot([y_true.min(), y_true.max()], \n",
    "                                           [y_true.min(), y_true.max()], \n",
    "                                           'r--', lw=2)\n",
    "                            axes[0, 0].set_xlabel('Actual')\n",
    "                            axes[0, 0].set_ylabel('Predicted')\n",
    "                            axes[0, 0].set_title(f'Predictions vs Actual ({target_col})')\n",
    "                            axes[0, 0].grid(True, alpha=0.3)\n",
    "                            \n",
    "                            # Residual plot\n",
    "                            axes[0, 1].scatter(y_pred, pred_df['Residual'], alpha=0.5, color='green')\n",
    "                            axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "                            axes[0, 1].set_xlabel('Predicted')\n",
    "                            axes[0, 1].set_ylabel('Residuals')\n",
    "                            axes[0, 1].set_title('Residual Plot')\n",
    "                            axes[0, 1].grid(True, alpha=0.3)\n",
    "                            \n",
    "                            # Error distribution\n",
    "                            axes[1, 0].hist(pred_df['Absolute_Error'], bins=30, \n",
    "                                           edgecolor='black', alpha=0.7, color='orange')\n",
    "                            axes[1, 0].axvline(mae, color='red', linestyle='--', \n",
    "                                             label=f'Mean AE: {mae:.4f}')\n",
    "                            axes[1, 0].set_xlabel('Absolute Error')\n",
    "                            axes[1, 0].set_ylabel('Frequency')\n",
    "                            axes[1, 0].set_title('Absolute Error Distribution')\n",
    "                            axes[1, 0].legend()\n",
    "                            axes[1, 0].grid(True, alpha=0.3)\n",
    "                            \n",
    "                            # Prediction error by actual value\n",
    "                            axes[1, 1].scatter(y_true, pred_df['Absolute_Error'], \n",
    "                                             alpha=0.5, color='purple')\n",
    "                            axes[1, 1].set_xlabel('Actual Value')\n",
    "                            axes[1, 1].set_ylabel('Absolute Error')\n",
    "                            axes[1, 1].set_title('Error by Actual Value')\n",
    "                            axes[1, 1].grid(True, alpha=0.3)\n",
    "                            \n",
    "                            plt.tight_layout()\n",
    "                            plt.show()\n",
    "                            \n",
    "                            # Display worst predictions\n",
    "                            print(\"\\n‚ö†Ô∏è Top 10 Worst Predictions (Highest Absolute Error):\")\n",
    "                            worst_preds = pred_df.nlargest(10, 'Absolute_Error')\n",
    "                            worst_preds_display = worst_preds.copy()\n",
    "                            worst_preds_display['Actual'] = worst_preds_display['Actual'].apply(lambda x: f\"{x:.2f}\")\n",
    "                            worst_preds_display['Predicted'] = worst_preds_display['Predicted'].apply(lambda x: f\"{x:.2f}\")\n",
    "                            worst_preds_display['Error'] = worst_preds_display['Absolute_Error'].apply(lambda x: f\"{x:.2f}\")\n",
    "                            display(worst_preds_display[['Actual', 'Predicted', 'Error']])\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error analyzing predictions: {e}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "                else:\n",
    "                    print(f\"Best model {best_model_name} not found at {model_path}\")\n",
    "            else:\n",
    "                print(\"Could not determine best model from comparison results\")\n",
    "        else:\n",
    "            print(\"No target variables found in test data\")\n",
    "    else:\n",
    "        print(f\"Test data not found at {test_path}\")\n",
    "\n",
    "# Run prediction analysis\n",
    "analyze_predictions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191dc9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 12. Business Impact Analysis\n",
    "\n",
    "# %%\n",
    "print(\"=\"*60)\n",
    "print(\"BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate potential business impact\n",
    "if 'claim_policies' in data_dict:\n",
    "    df = data_dict['claim_policies']\n",
    "    \n",
    "    # Get best model metrics\n",
    "    best_model_name = None\n",
    "    best_r2 = None\n",
    "    best_rmse = None\n",
    "    \n",
    "    if 'model_comparison' in model_results:\n",
    "        try:\n",
    "            model_data = model_results['model_comparison']\n",
    "            if isinstance(model_data, dict):\n",
    "                # Find best model by R¬≤\n",
    "                best_r2 = -np.inf\n",
    "                for model, metrics in model_data.items():\n",
    "                    if isinstance(metrics, dict):\n",
    "                        r2 = metrics.get('r2', metrics.get('R2', -np.inf))\n",
    "                        if r2 > best_r2:\n",
    "                            best_r2 = r2\n",
    "                            best_model_name = model\n",
    "                            rmse = metrics.get('rmse', metrics.get('RMSE', None))\n",
    "                            if rmse:\n",
    "                                best_rmse = rmse\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Also check the 'best_model' key\n",
    "    if not best_model_name and 'best_model' in model_results:\n",
    "        best_model_name = model_results['best_model']\n",
    "    \n",
    "    # Business metrics\n",
    "    if 'TotalPremium' in df.columns and 'TotalClaims' in df.columns:\n",
    "        total_premium = df['TotalPremium'].sum()\n",
    "        total_claims = df['TotalClaims'].sum()\n",
    "        avg_claim = df['TotalClaims'].mean()\n",
    "        loss_ratio = total_claims / total_premium * 100 if total_premium > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüí∞ Business Metrics:\")\n",
    "        print(f\"  Total Premium: R{total_premium:,.2f}\")\n",
    "        print(f\"  Total Claims: R{total_claims:,.2f}\")\n",
    "        print(f\"  Average Claim: R{avg_claim:,.2f}\")\n",
    "        print(f\"  Loss Ratio: {loss_ratio:.2f}%\")\n",
    "        \n",
    "        if best_model_name:\n",
    "            print(f\"\\nüéØ Best Model: {best_model_name}\")\n",
    "            \n",
    "            if best_r2:\n",
    "                print(f\"  R¬≤ Score: {best_r2:.4f}\")\n",
    "                if best_rmse:\n",
    "                    print(f\"  Prediction Error (RMSE): R{best_rmse:,.2f}\")\n",
    "                \n",
    "                # Potential impact\n",
    "                print(\"\\nüìà Potential Business Impact:\")\n",
    "                if best_r2 > 0.5:\n",
    "                    print(f\"  ‚Ä¢ Model explains {best_r2*100:.1f}% of claim variability\")\n",
    "                    if best_rmse:\n",
    "                        print(f\"  ‚Ä¢ Average prediction error: R{best_rmse:,.2f}\")\n",
    "                    print(f\"  ‚Ä¢ Better pricing accuracy could improve profitability\")\n",
    "                elif best_r2 > 0.3:\n",
    "                    print(f\"  ‚Ä¢ Model explains {best_r2*100:.1f}% of claim variability\")\n",
    "                    print(f\"  ‚Ä¢ Moderate predictive power - useful for risk assessment\")\n",
    "                else:\n",
    "                    print(f\"  ‚Ä¢ Model explains only {best_r2*100:.1f}% of claim variability\")\n",
    "                    print(f\"  ‚Ä¢ Consider feature engineering or alternative approaches\")\n",
    "            else:\n",
    "                print(\"  R¬≤ score not available for best model\")\n",
    "            \n",
    "            if loss_ratio > 100:\n",
    "                print(f\"  ‚ö†Ô∏è  High loss ratio ({loss_ratio:.1f}%) indicates underpricing\")\n",
    "                print(f\"  ‚úÖ Model can help identify optimal premium levels\")\n",
    "            elif loss_ratio < 70:\n",
    "                print(f\"  ‚úÖ Good loss ratio ({loss_ratio:.1f}%) indicates healthy margins\")\n",
    "                print(f\"  üìä Model can help maintain competitive pricing\")\n",
    "            else:\n",
    "                print(f\"  üìä Loss ratio ({loss_ratio:.1f}%) within typical range\")\n",
    "                print(f\"  üîç Model can optimize for profitability and growth\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Model Performance Note:\")\n",
    "            print(\"  Could not determine best model from results\")\n",
    "    else:\n",
    "        print(\"Required columns (TotalPremium, TotalClaims) not found in data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba59340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 13. Export Summary Report\n",
    "\n",
    "# %%\n",
    "def create_summary_report():\n",
    "    \"\"\"Create a comprehensive summary report\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"SUMMARY REPORT GENERATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    report_data = {\n",
    "        \"timestamp\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"data_statistics\": {},\n",
    "        \"model_performance\": {},\n",
    "        \"best_model\": {},\n",
    "        \"business_insights\": []\n",
    "    }\n",
    "    \n",
    "    # Data statistics\n",
    "    if 'claim_policies' in data_dict:\n",
    "        df = data_dict['claim_policies']\n",
    "        report_data[\"data_statistics\"] = {\n",
    "            \"total_records\": len(df),\n",
    "            \"total_features\": len(df.columns)\n",
    "        }\n",
    "        \n",
    "        if 'TotalPremium' in df.columns:\n",
    "            report_data[\"data_statistics\"][\"total_premium\"] = float(df['TotalPremium'].sum())\n",
    "        \n",
    "        if 'TotalClaims' in df.columns:\n",
    "            report_data[\"data_statistics\"][\"total_claims\"] = float(df['TotalClaims'].sum())\n",
    "            report_data[\"data_statistics\"][\"average_claim\"] = float(df['TotalClaims'].mean())\n",
    "            report_data[\"data_statistics\"][\"max_claim\"] = float(df['TotalClaims'].max())\n",
    "            report_data[\"data_statistics\"][\"min_claim\"] = float(df['TotalClaims'].min())\n",
    "    \n",
    "    # Model performance\n",
    "    if 'model_comparison' in model_results:\n",
    "        model_data = model_results['model_comparison']\n",
    "        report_data[\"model_performance\"] = model_data\n",
    "        \n",
    "        # Try to find best model\n",
    "        if isinstance(model_data, dict):\n",
    "            best_model = None\n",
    "            best_r2 = -np.inf\n",
    "            \n",
    "            for model_name, metrics in model_data.items():\n",
    "                if isinstance(metrics, dict):\n",
    "                    r2 = metrics.get('r2', metrics.get('R2', -np.inf))\n",
    "                    if r2 > best_r2:\n",
    "                        best_r2 = r2\n",
    "                        best_model = model_name\n",
    "            \n",
    "            if best_model:\n",
    "                report_data[\"best_model\"] = {\n",
    "                    \"name\": best_model,\n",
    "                    \"r2\": float(best_r2) if best_r2 != -np.inf else None\n",
    "                }\n",
    "                \n",
    "                if isinstance(model_data[best_model], dict):\n",
    "                    metrics = model_data[best_model]\n",
    "                    if 'mae' in metrics or 'MAE' in metrics:\n",
    "                        mae = metrics.get('mae', metrics.get('MAE'))\n",
    "                        if mae is not None:\n",
    "                            report_data[\"best_model\"][\"mae\"] = float(mae)\n",
    "                    if 'rmse' in metrics or 'RMSE' in metrics:\n",
    "                        rmse = metrics.get('rmse', metrics.get('RMSE'))\n",
    "                        if rmse is not None:\n",
    "                            report_data[\"best_model\"][\"rmse\"] = float(rmse)\n",
    "                    if 'training_time' in metrics or 'Training_Time' in metrics:\n",
    "                        train_time = metrics.get('training_time', metrics.get('Training_Time'))\n",
    "                        if train_time is not None:\n",
    "                            report_data[\"best_model\"][\"training_time\"] = float(train_time)\n",
    "    \n",
    "    # Also check the explicit best_model key\n",
    "    if 'best_model' in model_results and not report_data[\"best_model\"]:\n",
    "        report_data[\"best_model\"][\"name\"] = model_results['best_model']\n",
    "    \n",
    "    # Business insights\n",
    "    if 'task4_report' in model_results:\n",
    "        report = model_results['task4_report']\n",
    "        if 'key_insights' in report:\n",
    "            report_data[\"business_insights\"] = report['key_insights']\n",
    "        elif 'insights' in report and isinstance(report['insights'], list):\n",
    "            report_data[\"business_insights\"] = report['insights']\n",
    "    \n",
    "    # Save report\n",
    "    report_path = RESULTS_PATH / \"modeling_eda_summary.json\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report_data, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Summary report saved to: {report_path}\")\n",
    "    \n",
    "    # Display report summary\n",
    "    print(\"\\nüìã Report Summary:\")\n",
    "    print(f\"  Generated: {report_data['timestamp']}\")\n",
    "    print(f\"  Records analyzed: {report_data['data_statistics'].get('total_records', 'N/A')}\")\n",
    "    print(f\"  Best model: {report_data['best_model'].get('name', 'N/A')}\")\n",
    "    if report_data['best_model'].get('r2'):\n",
    "        print(f\"  Best R¬≤: {report_data['best_model']['r2']:.4f}\")\n",
    "    \n",
    "    return report_data\n",
    "\n",
    "# Generate summary report\n",
    "summary_report = create_summary_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ef3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 14. Interactive Dashboard (Optional - requires Plotly)\n",
    "\n",
    "# %%\n",
    "# Create an interactive dashboard if plotly is available\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"INTERACTIVE DASHBOARD\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'model_comparison' in model_results:\n",
    "        # Try to create comparison DataFrame\n",
    "        model_data = model_results['model_comparison']\n",
    "        \n",
    "        if isinstance(model_data, dict):\n",
    "            # Extract data for plotting\n",
    "            plot_data = []\n",
    "            for model_name, metrics in model_data.items():\n",
    "                if isinstance(metrics, dict):\n",
    "                    r2 = metrics.get('r2', metrics.get('R2', 0))\n",
    "                    if r2 != 0:  # Only include models with R2 data\n",
    "                        plot_data.append({\n",
    "                            'Model': model_name,\n",
    "                            'R2': r2,\n",
    "                            'MAE': metrics.get('mae', metrics.get('MAE', 0)),\n",
    "                            'RMSE': metrics.get('rmse', metrics.get('RMSE', 0))\n",
    "                        })\n",
    "            \n",
    "            if plot_data:\n",
    "                comp_df = pd.DataFrame(plot_data)\n",
    "                \n",
    "                # Create interactive bar chart\n",
    "                fig = px.bar(comp_df, \n",
    "                             x='Model', \n",
    "                             y='R2',\n",
    "                             title='Model R¬≤ Scores - Interactive View',\n",
    "                             color='R2',\n",
    "                             color_continuous_scale='Viridis',\n",
    "                             text='R2')\n",
    "                \n",
    "                fig.update_traces(texttemplate='%{text:.3f}', textposition='outside')\n",
    "                fig.update_layout(xaxis_title='Model',\n",
    "                                 yaxis_title='R¬≤ Score',\n",
    "                                 showlegend=False)\n",
    "                \n",
    "                fig.show()\n",
    "                \n",
    "                # Create scatter plot of R2 vs RMSE\n",
    "                fig2 = px.scatter(comp_df,\n",
    "                                 x='R2',\n",
    "                                 y='RMSE',\n",
    "                                 text='Model',\n",
    "                                 title='Model Performance: R¬≤ vs RMSE',\n",
    "                                 labels={'R2': 'R¬≤ Score', 'RMSE': 'Root Mean Squared Error'})\n",
    "                \n",
    "                fig2.update_traces(textposition='top center', marker=dict(size=12))\n",
    "                fig2.show()\n",
    "                \n",
    "            else:\n",
    "                print(\"No model data available for interactive visualization\")\n",
    "    \n",
    "    # Try to create scatter plot from data\n",
    "    if 'claim_policies' in data_dict:\n",
    "        df = data_dict['claim_policies']\n",
    "        if 'TotalClaims' in df.columns and 'VehicleAge' in df.columns:\n",
    "            # Sample data for visualization (avoid too many points)\n",
    "            sample_size = min(1000, len(df))\n",
    "            sample_df = df.sample(sample_size) if sample_size < len(df) else df\n",
    "            \n",
    "            fig3 = px.scatter(sample_df,\n",
    "                             x='VehicleAge',\n",
    "                             y='TotalClaims',\n",
    "                             color='TotalClaims',\n",
    "                             size='SumInsured' if 'SumInsured' in df.columns else None,\n",
    "                             hover_data=['VehicleType', 'Province'] if 'VehicleType' in df.columns and 'Province' in df.columns else None,\n",
    "                             title='Claims by Vehicle Age and Sum Insured',\n",
    "                             labels={'TotalClaims': 'Claim Amount (R)', \n",
    "                                    'VehicleAge': 'Vehicle Age (years)'})\n",
    "            fig3.show()\n",
    "                \n",
    "except ImportError:\n",
    "    print(\"\\n‚ÑπÔ∏è Plotly not available for interactive visualizations\")\n",
    "    print(\"Install with: pip install plotly\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError creating interactive visualizations: {e}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 15. Conclusion and Next Steps\n",
    "\n",
    "# %%\n",
    "print(\"=\"*60)\n",
    "print(\"CONCLUSION AND NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüéØ Key Findings:\")\n",
    "print(\"1. Data Quality: Check missing values and data types above\")\n",
    "print(\"2. Model Performance: Review model comparison metrics\")\n",
    "print(\"3. Best Model: Identify from the comparison results\")\n",
    "print(\"4. Feature Importance: Check which features drive predictions\")\n",
    "\n",
    "print(\"\\nüöÄ Recommended Next Steps:\")\n",
    "print(\"1. Review model performance and select best model for deployment\")\n",
    "print(\"2. Validate predictions on new/unseen data\")\n",
    "print(\"3. Implement model monitoring for production\")\n",
    "print(\"4. Conduct feature engineering to improve model performance\")\n",
    "print(\"5. Develop business rules based on model insights\")\n",
    "\n",
    "print(\"\\nüìä Additional Analyses to Consider:\")\n",
    "print(\"‚Ä¢ Time-series analysis of claim patterns\")\n",
    "print(\"‚Ä¢ Geospatial analysis of claim hotspots\")\n",
    "print(\"‚Ä¢ Customer segmentation based on risk profiles\")\n",
    "print(\"‚Ä¢ Cost-benefit analysis of model implementation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ MODELING EDA COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 16. Debug Information (Optional)\n",
    "\n",
    "# %%\n",
    "# Optional: Print debug information about data structures\n",
    "print(\"=\"*60)\n",
    "print(\"DEBUG INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìÅ Data files loaded: {list(data_dict.keys())}\")\n",
    "\n",
    "print(f\"\\nüìä Model results loaded:\")\n",
    "for key in model_results.keys():\n",
    "    if key != 'task4_md':  # Don't print large markdown\n",
    "        print(f\"  - {key}: {type(model_results[key])}\")\n",
    "\n",
    "# Check model_comparison structure in detail\n",
    "if 'model_comparison' in model_results:\n",
    "    print(\"\\nüîç Detailed model_comparison structure:\")\n",
    "    model_data = model_results['model_comparison']\n",
    "    if isinstance(model_data, dict):\n",
    "        print(f\"  Number of models: {len(model_data)}\")\n",
    "        print(f\"  Models: {list(model_data.keys())}\")\n",
    "        \n",
    "        # Show structure of first few models\n",
    "        for i, (model_name, metrics) in enumerate(list(model_data.items())[:3]):\n",
    "            print(f\"\\n  Model {i+1}: {model_name}\")\n",
    "            if isinstance(metrics, dict):\n",
    "                print(f\"    Type: dict with {len(metrics)} keys\")\n",
    "                print(f\"    Keys: {list(metrics.keys())}\")\n",
    "                # Show sample values\n",
    "                for key, value in list(metrics.items())[:3]:\n",
    "                    print(f\"      {key}: {value}\")\n",
    "                if len(metrics) > 3:\n",
    "                    print(f\"      ... and {len(metrics) - 3} more\")\n",
    "            else:\n",
    "                print(f\"    Type: {type(metrics)}\")\n",
    "                print(f\"    Value: {metrics}\")\n",
    "\n",
    "# Check if model files exist\n",
    "print(\"\\nüîç Checking model files:\")\n",
    "model_files = list(MODELS_PATH.glob(\"*.pkl\"))\n",
    "print(f\"  Found {len(model_files)} .pkl files\")\n",
    "for file in model_files[:10]:  # Show first 10\n",
    "    print(f\"    {file.name}\")\n",
    "if len(model_files) > 10:\n",
    "    print(f\"    ... and {len(model_files) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5121af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
