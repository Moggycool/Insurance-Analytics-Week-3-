{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fdcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Insurance Claim Severity Modeling - EDA & Results Dashboard\n",
    "# \n",
    "# This notebook provides a comprehensive overview of the insurance claim severity modeling pipeline, including data exploration, model results, and insights.\n",
    "# \n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Configuration\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up paths\n",
    "BASE_PATH = Path.cwd().parent\n",
    "DATA_PATH = BASE_PATH / \"data\"\n",
    "MODELS_PATH = BASE_PATH / \"models\"\n",
    "RESULTS_PATH = BASE_PATH / \"Results\"\n",
    "NOTEBOOKS_PATH = BASE_PATH / \"notebooks\"\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(BASE_PATH / \"src\"))\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Base path: {BASE_PATH}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Load Data\n",
    "\n",
    "# %%\n",
    "def load_data():\n",
    "    \"\"\"Load all available datasets\"\"\"\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Load processed data\n",
    "    try:\n",
    "        processed_path = DATA_PATH / \"processed\"\n",
    "        files = list(processed_path.glob(\"*.csv\"))\n",
    "        \n",
    "        for file in files:\n",
    "            name = file.stem\n",
    "            print(f\"Loading {name}...\")\n",
    "            try:\n",
    "                data_dict[name] = pd.read_csv(file)\n",
    "                print(f\"  Shape: {data_dict[name].shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {file}: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading processed data: {e}\")\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading datasets...\")\n",
    "data_dict = load_data()\n",
    "\n",
    "# Display available datasets\n",
    "print(\"\\nAvailable datasets:\")\n",
    "for name, df in data_dict.items():\n",
    "    print(f\"  - {name}: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Data Exploration\n",
    "\n",
    "# %%\n",
    "if 'claim_policies' in data_dict:\n",
    "    df = data_dict['claim_policies'].copy()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"DATA EXPLORATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nüìä Dataset Shape: {df.shape}\")\n",
    "    print(f\"üìã Columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nData Types:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing %': missing_pct\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        display(missing_df)\n",
    "    else:\n",
    "        print(\"  No missing values found!\")\n",
    "    \n",
    "    # Target variable analysis\n",
    "    if 'TotalClaims' in df.columns:\n",
    "        print(\"\\nüéØ Target Variable Analysis (TotalClaims):\")\n",
    "        target_stats = df['TotalClaims'].describe()\n",
    "        display(pd.DataFrame(target_stats).T)\n",
    "        \n",
    "        # Check for extreme values\n",
    "        print(f\"\\nüîç Extreme Values Check:\")\n",
    "        print(f\"  Min: R{df['TotalClaims'].min():,.2f}\")\n",
    "        print(f\"  Max: R{df['TotalClaims'].max():,.2f}\")\n",
    "        print(f\"  Mean: R{df['TotalClaims'].mean():,.2f}\")\n",
    "        print(f\"  Std: R{df['TotalClaims'].std():,.2f}\")\n",
    "        \n",
    "        # Check for zeros or negative values\n",
    "        zero_claims = (df['TotalClaims'] == 0).sum()\n",
    "        negative_claims = (df['TotalClaims'] < 0).sum()\n",
    "        print(f\"  Zero claims: {zero_claims} ({zero_claims/len(df)*100:.2f}%)\")\n",
    "        print(f\"  Negative claims: {negative_claims} ({negative_claims/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        # Plot target distribution\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Histogram\n",
    "        axes[0].hist(df['TotalClaims'], bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[0].axvline(df['TotalClaims'].mean(), color='red', linestyle='--', \n",
    "                       label=f'Mean: R{df[\"TotalClaims\"].mean():,.2f}')\n",
    "        axes[0].axvline(df['TotalClaims'].median(), color='green', linestyle='--',\n",
    "                       label=f'Median: R{df[\"TotalClaims\"].median():,.2f}')\n",
    "        axes[0].set_xlabel('Claim Amount (R)')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        axes[0].set_title('Distribution of Claim Amounts')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Log transformation\n",
    "        if (df['TotalClaims'] > 0).all():\n",
    "            log_claims = np.log1p(df['TotalClaims'])\n",
    "            axes[1].hist(log_claims, bins=50, edgecolor='black', alpha=0.7)\n",
    "            axes[1].set_xlabel('Log(1 + Claim Amount)')\n",
    "            axes[1].set_ylabel('Frequency')\n",
    "            axes[1].set_title('Log-Transformed Claim Amounts')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'Log transform not possible\\n(negative values present)',\n",
    "                        ha='center', va='center', transform=axes[1].transAxes)\n",
    "            axes[1].set_title('Log-Transformed Claim Amounts')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Top claims\n",
    "        print(\"\\nüí∞ Top 10 Largest Claims:\")\n",
    "        top_claims = df.nlargest(10, 'TotalClaims')[['PolicyID', 'TotalClaims']].copy()\n",
    "        top_claims['TotalClaims'] = top_claims['TotalClaims'].apply(lambda x: f'R{x:,.2f}')\n",
    "        display(top_claims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Load Model Results\n",
    "\n",
    "# %%\n",
    "def load_model_results():\n",
    "    \"\"\"Load model evaluation results\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Load model comparison\n",
    "    model_comp_path = MODELS_PATH / \"model_comparison.json\"\n",
    "    if model_comp_path.exists():\n",
    "        try:\n",
    "            with open(model_comp_path, 'r') as f:\n",
    "                model_data = json.load(f)\n",
    "            \n",
    "            print(f\"Model comparison data type: {type(model_data)}\")\n",
    "            \n",
    "            # Handle different structures\n",
    "            if isinstance(model_data, list):\n",
    "                print(f\"Model comparison is a list with {len(model_data)} items\")\n",
    "                results['model_comparison'] = model_data\n",
    "            elif isinstance(model_data, dict):\n",
    "                print(f\"Model comparison keys: {list(model_data.keys())}\")\n",
    "                \n",
    "                # Check if it has a nested structure\n",
    "                if 'model_comparison' in model_data:\n",
    "                    print(\"Found nested 'model_comparison' key\")\n",
    "                    nested_data = model_data['model_comparison']\n",
    "                    print(f\"Nested data type: {type(nested_data)}\")\n",
    "                    results['model_comparison'] = nested_data\n",
    "                    \n",
    "                    # Store other keys if present\n",
    "                    for key in ['detailed_metrics', 'best_model']:\n",
    "                        if key in model_data:\n",
    "                            results[key] = model_data[key]\n",
    "                else:\n",
    "                    results['model_comparison'] = model_data\n",
    "            \n",
    "            print(f\"‚úì Loaded model comparison results\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model comparison: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Load cross-validation results\n",
    "    cv_path = MODELS_PATH / \"cross_validation_results.json\"\n",
    "    if cv_path.exists():\n",
    "        with open(cv_path, 'r') as f:\n",
    "            results['cv_results'] = json.load(f)\n",
    "        print(f\"‚úì Loaded cross-validation results\")\n",
    "    \n",
    "    # Load Lasso best params\n",
    "    lasso_params_path = MODELS_PATH / \"Lasso_best_params.json\"\n",
    "    if lasso_params_path.exists():\n",
    "        with open(lasso_params_path, 'r') as f:\n",
    "            results['lasso_params'] = json.load(f)\n",
    "        print(f\"‚úì Loaded Lasso parameters\")\n",
    "    \n",
    "    # Load Linear Regression best params\n",
    "    lr_params_path = MODELS_PATH / \"LinearRegression_best_params.json\"\n",
    "    if lr_params_path.exists():\n",
    "        with open(lr_params_path, 'r') as f:\n",
    "            results['lr_params'] = json.load(f)\n",
    "        print(f\"‚úì Loaded Linear Regression parameters\")\n",
    "    \n",
    "    # Load task 4 reports\n",
    "    task4_json_path = RESULTS_PATH / \"Task4_Reports\" / \"task4_comprehensive_report.json\"\n",
    "    if task4_json_path.exists():\n",
    "        with open(task4_json_path, 'r') as f:\n",
    "            results['task4_report'] = json.load(f)\n",
    "        print(f\"‚úì Loaded Task 4 comprehensive report\")\n",
    "    \n",
    "    task4_md_path = RESULTS_PATH / \"Task4_Reports\" / \"task4_final_report.md\"\n",
    "    if task4_md_path.exists():\n",
    "        with open(task4_md_path, 'r') as f:\n",
    "            results['task4_md'] = f.read()\n",
    "        print(f\"‚úì Loaded Task 4 markdown report\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Loading model results...\")\n",
    "model_results = load_model_results()\n",
    "\n",
    "# Display what we loaded\n",
    "print(\"\\nüìä Loaded model results:\")\n",
    "for key, value in model_results.items():\n",
    "    if key != 'task4_md':  # Don't print large markdown content\n",
    "        print(f\"  {key}: {type(value)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828c1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Model Performance Analysis\n",
    "\n",
    "# %%\n",
    "if 'model_comparison' in model_results:\n",
    "    print(\"=\"*60)\n",
    "    print(\"MODEL PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model_data = model_results['model_comparison']\n",
    "    print(f\"Model comparison data type: {type(model_data)}\")\n",
    "    \n",
    "    if isinstance(model_data, list):\n",
    "        print(f\"Model comparison is a list with {len(model_data)} items\")\n",
    "        print(\"\\nFirst item in list:\")\n",
    "        print(f\"  Type: {type(model_data[0])}\")\n",
    "        if isinstance(model_data[0], dict):\n",
    "            print(f\"  Keys: {list(model_data[0].keys())}\")\n",
    "    \n",
    "    # Check for detailed_metrics separately\n",
    "    if 'detailed_metrics' in model_results:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DETAILED MODEL METRICS ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        detailed_metrics = model_results['detailed_metrics']\n",
    "        print(f\"Detailed metrics type: {type(detailed_metrics)}\")\n",
    "        \n",
    "        if isinstance(detailed_metrics, dict):\n",
    "            print(f\"Number of models in detailed metrics: {len(detailed_metrics)}\")\n",
    "            print(f\"Models: {list(detailed_metrics.keys())}\")\n",
    "            \n",
    "            # Analyze metrics for each model\n",
    "            model_analysis = []\n",
    "            \n",
    "            for model_name, metrics in detailed_metrics.items():\n",
    "                if isinstance(metrics, dict):\n",
    "                    print(f\"\\nüìä {model_name}:\")\n",
    "                    \n",
    "                    # Check for problematic values\n",
    "                    problem_flags = []\n",
    "                    \n",
    "                    for metric_name, value in metrics.items():\n",
    "                        if pd.isna(value) or np.isinf(value) or (isinstance(value, (int, float)) and abs(value) > 1e100):\n",
    "                            problem_flags.append(metric_name)\n",
    "                            print(f\"  ‚ö†Ô∏è  {metric_name}: {value} (PROBLEMATIC)\")\n",
    "                        else:\n",
    "                            print(f\"  ‚úì {metric_name}: {value}\")\n",
    "                    \n",
    "                    model_analysis.append({\n",
    "                        'Model': model_name,\n",
    "                        'Has_Problems': len(problem_flags) > 0,\n",
    "                        'Problem_Metrics': problem_flags,\n",
    "                        'R2': metrics.get('r2', metrics.get('R2', np.nan)),\n",
    "                        'MAE': metrics.get('mae', metrics.get('MAE', np.nan)),\n",
    "                        'RMSE': metrics.get('rmse', metrics.get('RMSE', np.nan))\n",
    "                    })\n",
    "            \n",
    "            # Create summary DataFrame\n",
    "            if model_analysis:\n",
    "                analysis_df = pd.DataFrame(model_analysis)\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"MODEL PROBLEM SUMMARY\")\n",
    "                print(\"=\"*60)\n",
    "                display(analysis_df)\n",
    "                \n",
    "                # Count problematic models\n",
    "                problematic_models = analysis_df[analysis_df['Has_Problems']]\n",
    "                print(f\"\\n‚ö†Ô∏è  {len(problematic_models)} models have problematic metrics\")\n",
    "                print(f\"‚úÖ {len(analysis_df) - len(problematic_models)} models have valid metrics\")\n",
    "                \n",
    "                # Show models without problems\n",
    "                valid_models = analysis_df[~analysis_df['Has_Problems']]\n",
    "                if len(valid_models) > 0:\n",
    "                    print(\"\\n‚úÖ Models with valid metrics:\")\n",
    "                    for _, row in valid_models.iterrows():\n",
    "                        print(f\"  - {row['Model']}: R¬≤={row['R2']:.4f}, MAE={row['MAE']:.2f}, RMSE={row['RMSE']:.2f}\")\n",
    "                \n",
    "                # Show models with problems\n",
    "                if len(problematic_models) > 0:\n",
    "                    print(\"\\n‚ö†Ô∏è  Models with problematic metrics:\")\n",
    "                    for _, row in problematic_models.iterrows():\n",
    "                        print(f\"  - {row['Model']}: Problems in {row['Problem_Metrics']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2762d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Debug Model Issues\n",
    "\n",
    "# %%\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL ISSUE DEBUGGING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if we have train/test data\n",
    "print(\"\\nüîç Checking for training and test data...\")\n",
    "train_path = DATA_PATH / \"processed\" / \"train_data.csv\"\n",
    "test_path = DATA_PATH / \"processed\" / \"test_data.csv\"\n",
    "\n",
    "if train_path.exists() and test_path.exists():\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "    \n",
    "    # Check target variable\n",
    "    target_cols = ['Log_TotalClaims', 'TotalClaims']\n",
    "    available_targets = [col for col in target_cols if col in train_df.columns]\n",
    "    \n",
    "    if available_targets:\n",
    "        target_col = available_targets[0]\n",
    "        print(f\"\\nüéØ Target variable: {target_col}\")\n",
    "        \n",
    "        # Check target distribution\n",
    "        y_train = train_df[target_col]\n",
    "        y_test = test_df[target_col]\n",
    "        \n",
    "        print(f\"Training target stats:\")\n",
    "        print(f\"  Min: {y_train.min():.2f}\")\n",
    "        print(f\"  Max: {y_train.max():.2f}\")\n",
    "        print(f\"  Mean: {y_train.mean():.2f}\")\n",
    "        print(f\"  Std: {y_train.std():.2f}\")\n",
    "        \n",
    "        print(f\"\\nTest target stats:\")\n",
    "        print(f\"  Min: {y_test.min():.2f}\")\n",
    "        print(f\"  Max: {y_test.max():.2f}\")\n",
    "        print(f\"  Mean: {y_test.mean():.2f}\")\n",
    "        print(f\"  Std: {y_test.std():.2f}\")\n",
    "        \n",
    "        # Check for extreme values\n",
    "        train_extreme = (y_train.abs() > 1e10).sum()\n",
    "        test_extreme = (y_test.abs() > 1e10).sum()\n",
    "        \n",
    "        if train_extreme > 0 or test_extreme > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Found extreme values in target!\")\n",
    "            print(f\"  Training: {train_extreme} extreme values\")\n",
    "            print(f\"  Test: {test_extreme} extreme values\")\n",
    "            \n",
    "            # Show extreme values\n",
    "            if train_extreme > 0:\n",
    "                print(f\"\\n  Training extreme values:\")\n",
    "                extreme_train = y_train[y_train.abs() > 1e10]\n",
    "                print(f\"    {extreme_train.head()}\")\n",
    "            \n",
    "            if test_extreme > 0:\n",
    "                print(f\"\\n  Test extreme values:\")\n",
    "                extreme_test = y_test[y_test.abs() > 1e10]\n",
    "                print(f\"    {extreme_test.head()}\")\n",
    "    else:\n",
    "        print(\"‚ùå No target variables found in training data\")\n",
    "else:\n",
    "    print(\"‚ùå Training or test data not found\")\n",
    "\n",
    "# Check feature columns\n",
    "print(\"\\nüîç Checking feature columns...\")\n",
    "if train_path.exists():\n",
    "    # Get feature columns (exclude target columns)\n",
    "    feature_cols = [col for col in train_df.columns \n",
    "                   if col not in ['Log_TotalClaims', 'TotalClaims', 'HighClaim']]\n",
    "    \n",
    "    print(f\"Number of features: {len(feature_cols)}\")\n",
    "    print(f\"First 10 features: {feature_cols[:10]}\")\n",
    "    \n",
    "    # Check for NaN/inf in features\n",
    "    print(\"\\nüîç Checking for NaN/Inf in features...\")\n",
    "    for col in feature_cols[:5]:  # Check first 5 features\n",
    "        if col in train_df.columns:\n",
    "            nan_count = train_df[col].isna().sum()\n",
    "            inf_count = np.isinf(train_df[col]).sum() if train_df[col].dtype in ['float64', 'int64'] else 0\n",
    "            if nan_count > 0 or inf_count > 0:\n",
    "                print(f\"  ‚ö†Ô∏è  {col}: {nan_count} NaN, {inf_count} Inf\")\n",
    "            else:\n",
    "                print(f\"  ‚úì {col}: No NaN/Inf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Try to Load and Test a Single Model\n",
    "\n",
    "# %%\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING INDIVIDUAL MODEL LOADING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to load a simple model\n",
    "test_model_name = \"LinearRegression\"\n",
    "model_path = MODELS_PATH / f\"{test_model_name}.pkl\"\n",
    "\n",
    "if model_path.exists():\n",
    "    print(f\"Testing {test_model_name} model...\")\n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded {test_model_name} model\")\n",
    "        print(f\"  Model type: {type(model)}\")\n",
    "        \n",
    "        # Check if model has been fitted\n",
    "        if hasattr(model, 'coef_'):\n",
    "            print(f\"  Model has coefficients: {len(model.coef_) if hasattr(model.coef_, '__len__') else 1}\")\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            print(f\"  Model has feature importances\")\n",
    "        \n",
    "        # Try to make a prediction\n",
    "        if train_path.exists() and test_path.exists():\n",
    "            # Prepare features\n",
    "            X_train = train_df[feature_cols]\n",
    "            X_test = test_df[feature_cols]\n",
    "            \n",
    "            # Check if preprocessor exists\n",
    "            preprocessor_path = MODELS_PATH / \"preprocessor.pkl\"\n",
    "            if preprocessor_path.exists():\n",
    "                print(f\"\\nüîß Loading preprocessor...\")\n",
    "                with open(preprocessor_path, 'rb') as f:\n",
    "                    preprocessor = pickle.load(f)\n",
    "                \n",
    "                print(f\"‚úÖ Preprocessor loaded\")\n",
    "                \n",
    "                # Transform features\n",
    "                try:\n",
    "                    X_train_transformed = preprocessor.transform(X_train)\n",
    "                    X_test_transformed = preprocessor.transform(X_test)\n",
    "                    \n",
    "                    print(f\"‚úÖ Features transformed successfully\")\n",
    "                    print(f\"  Training features shape: {X_train_transformed.shape}\")\n",
    "                    print(f\"  Test features shape: {X_test_transformed.shape}\")\n",
    "                    \n",
    "                    # Make prediction\n",
    "                    try:\n",
    "                        y_pred = model.predict(X_test_transformed[:5])  # Predict on first 5 samples\n",
    "                        print(f\"\\n‚úÖ Made predictions on 5 samples:\")\n",
    "                        print(f\"  Predictions: {y_pred}\")\n",
    "                        \n",
    "                        # Compare with actual\n",
    "                        if available_targets:\n",
    "                            y_actual = test_df[target_col].values[:5]\n",
    "                            print(f\"  Actual values: {y_actual}\")\n",
    "                            print(f\"  Differences: {y_pred - y_actual}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Error making predictions: {e}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error transforming features: {e}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Preprocessor not found at {preprocessor_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\"‚ùå Model {test_model_name} not found at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df280bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Check Cross-Validation Results\n",
    "\n",
    "# %%\n",
    "if 'cv_results' in model_results:\n",
    "    print(\"=\"*60)\n",
    "    print(\"CROSS-VALIDATION RESULTS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    cv_data = model_results['cv_results']\n",
    "    print(f\"CV data type: {type(cv_data)}\")\n",
    "    \n",
    "    if isinstance(cv_data, dict):\n",
    "        print(f\"Number of models in CV: {len(cv_data)}\")\n",
    "        \n",
    "        # Analyze each model's CV results\n",
    "        cv_summary = []\n",
    "        \n",
    "        for model_name, scores in cv_data.items():\n",
    "            print(f\"\\nüìä {model_name}:\")\n",
    "            \n",
    "            if isinstance(scores, dict):\n",
    "                # Check for problematic values\n",
    "                model_issues = []\n",
    "                valid_metrics = []\n",
    "                \n",
    "                for metric_name, values in scores.items():\n",
    "                    if isinstance(values, list):\n",
    "                        # Check each value in the list\n",
    "                        problem_values = []\n",
    "                        for val in values:\n",
    "                            if pd.isna(val) or np.isinf(val) or (isinstance(val, (int, float)) and abs(val) > 1e100):\n",
    "                                problem_values.append(val)\n",
    "                        \n",
    "                        if problem_values:\n",
    "                            model_issues.append(f\"{metric_name} has {len(problem_values)} problematic values\")\n",
    "                        else:\n",
    "                            valid_metrics.append(metric_name)\n",
    "                \n",
    "                if model_issues:\n",
    "                    print(f\"  ‚ö†Ô∏è  Issues: {', '.join(model_issues)}\")\n",
    "                if valid_metrics:\n",
    "                    print(f\"  ‚úÖ Valid metrics: {', '.join(valid_metrics)}\")\n",
    "                \n",
    "                cv_summary.append({\n",
    "                    'Model': model_name,\n",
    "                    'Has_Issues': len(model_issues) > 0,\n",
    "                    'Issues': model_issues,\n",
    "                    'Valid_Metrics': valid_metrics\n",
    "                })\n",
    "        \n",
    "        # Create summary\n",
    "        if cv_summary:\n",
    "            summary_df = pd.DataFrame(cv_summary)\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"CROSS-VALIDATION SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Root Cause Analysis\n",
    "\n",
    "# %%\n",
    "print(\"=\"*60)\n",
    "print(\"ROOT CAUSE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüîç Based on the analysis, here are potential issues:\")\n",
    "print(\"\\n1. **Data Issues**:\")\n",
    "print(\"   ‚Ä¢ Target variable may have extreme values\")\n",
    "print(\"   ‚Ä¢ Features may contain NaN or infinite values\")\n",
    "print(\"   ‚Ä¢ Data may not be properly scaled\")\n",
    "\n",
    "print(\"\\n2. **Model Training Issues**:\")\n",
    "print(\"   ‚Ä¢ Models may not have converged properly\")\n",
    "print(\"   ‚Ä¢ Hyperparameters may be poorly chosen\")\n",
    "print(\"   ‚Ä¢ Data leakage between train and test sets\")\n",
    "\n",
    "print(\"\\n3. **Evaluation Issues**:\")\n",
    "print(\"   ‚Ä¢ Metrics calculation may have errors\")\n",
    "print(\"   ‚Ä¢ Predictions may be extremely large\")\n",
    "print(\"   ‚Ä¢ Log transformation issues\")\n",
    "\n",
    "print(\"\\nüöÄ **Recommended Fixes**:\")\n",
    "print(\"\\n1. **Check Data Preparation**:\")\n",
    "print(\"   - Verify that Log_TotalClaims was calculated correctly\")\n",
    "print(\"   - Check for and remove extreme outliers\")\n",
    "print(\"   - Ensure proper feature scaling\")\n",
    "\n",
    "print(\"\\n2. **Retrain Models**:\")\n",
    "print(\"   - Use simpler models first (LinearRegression)\")\n",
    "print(\"   - Add regularization (Ridge, Lasso)\")\n",
    "print(\"   - Limit tree depths for tree-based models\")\n",
    "\n",
    "print(\"\\n3. **Debug Step-by-Step**:\")\n",
    "print(\"   - Train one model at a time\")\n",
    "print(\"   - Check predictions after each step\")\n",
    "print(\"   - Verify metric calculations manually\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Create Diagnostic Report\n",
    "\n",
    "# %%\n",
    "def create_diagnostic_report():\n",
    "    \"\"\"Create a diagnostic report of issues found\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"DIAGNOSTIC REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    report = {\n",
    "        \"timestamp\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"issues_found\": [],\n",
    "        \"data_checks\": {},\n",
    "        \"model_checks\": {},\n",
    "        \"recommendations\": []\n",
    "    }\n",
    "    \n",
    "    # Data checks\n",
    "    if 'claim_policies' in data_dict:\n",
    "        df = data_dict['claim_policies']\n",
    "        report[\"data_checks\"][\"dataset_shape\"] = df.shape\n",
    "        \n",
    "        if 'TotalClaims' in df.columns:\n",
    "            target_stats = df['TotalClaims'].describe().to_dict()\n",
    "            report[\"data_checks\"][\"target_statistics\"] = target_stats\n",
    "            \n",
    "            # Check for extreme values\n",
    "            extreme_mask = (df['TotalClaims'].abs() > 1e10)\n",
    "            if extreme_mask.any():\n",
    "                report[\"issues_found\"].append(\"Extreme values in TotalClaims\")\n",
    "                report[\"data_checks\"][\"extreme_values_count\"] = int(extreme_mask.sum())\n",
    "    \n",
    "    # Model checks\n",
    "    if 'detailed_metrics' in model_results:\n",
    "        detailed_metrics = model_results['detailed_metrics']\n",
    "        report[\"model_checks\"][\"models_evaluated\"] = list(detailed_metrics.keys())\n",
    "        \n",
    "        # Check each model\n",
    "        problematic_models = []\n",
    "        for model_name, metrics in detailed_metrics.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                for metric_name, value in metrics.items():\n",
    "                    if pd.isna(value) or np.isinf(value) or (isinstance(value, (int, float)) and abs(value) > 1e100):\n",
    "                        problematic_models.append(model_name)\n",
    "                        break\n",
    "        \n",
    "        if problematic_models:\n",
    "            report[\"issues_found\"].append(f\"Problematic metrics in {len(problematic_models)} models\")\n",
    "            report[\"model_checks\"][\"problematic_models\"] = problematic_models\n",
    "    \n",
    "    # Recommendations\n",
    "    report[\"recommendations\"] = [\n",
    "        \"1. Check data preparation pipeline for errors\",\n",
    "        \"2. Verify target variable transformation (Log_TotalClaims)\",\n",
    "        \"3. Remove or cap extreme outliers\",\n",
    "        \"4. Retrain models with proper regularization\",\n",
    "        \"5. Start with simple LinearRegression as baseline\"\n",
    "    ]\n",
    "    \n",
    "    # Save report\n",
    "    report_path = RESULTS_PATH / \"model_diagnostic_report.json\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Diagnostic report saved to: {report_path}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nüìã Report Summary:\")\n",
    "    print(f\"  Issues found: {len(report['issues_found'])}\")\n",
    "    for issue in report['issues_found']:\n",
    "        print(f\"  ‚Ä¢ {issue}\")\n",
    "    \n",
    "    print(\"\\nüí° Recommendations:\")\n",
    "    for rec in report['recommendations']:\n",
    "        print(f\"  {rec}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Create diagnostic report\n",
    "diagnostic_report = create_diagnostic_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de379e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 11. Quick Fix: Recalculate Metrics Manually\n",
    "\n",
    "# %%\n",
    "print(\"=\"*60)\n",
    "print(\"QUICK FIX: MANUAL METRIC CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to recalculate metrics for one model\n",
    "if train_path.exists() and test_path.exists() and 'preprocessor.pkl' in [p.name for p in MODELS_PATH.glob('*.pkl')]:\n",
    "    print(\"\\nAttempting to recalculate metrics manually...\")\n",
    "    \n",
    "    # Load data\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # Get target and features\n",
    "    target_col = 'Log_TotalClaims' if 'Log_TotalClaims' in test_df.columns else 'TotalClaims'\n",
    "    feature_cols = [col for col in test_df.columns \n",
    "                   if col not in ['Log_TotalClaims', 'TotalClaims', 'HighClaim']]\n",
    "    \n",
    "    if target_col in test_df.columns and feature_cols:\n",
    "        X_test = test_df[feature_cols]\n",
    "        y_true = test_df[target_col]\n",
    "        \n",
    "        # Load preprocessor\n",
    "        with open(MODELS_PATH / \"preprocessor.pkl\", 'rb') as f:\n",
    "            preprocessor = pickle.load(f)\n",
    "        \n",
    "        # Transform features\n",
    "        X_test_transformed = preprocessor.transform(X_test)\n",
    "        \n",
    "        # Test each model\n",
    "        model_files = list(MODELS_PATH.glob(\"*.pkl\"))\n",
    "        model_files = [f for f in model_files if f.name != \"preprocessor.pkl\"]\n",
    "        \n",
    "        print(f\"\\nFound {len(model_files)} model files to test\")\n",
    "        \n",
    "        recalculated_metrics = {}\n",
    "        \n",
    "        for model_file in model_files[:3]:  # Test first 3 models\n",
    "            model_name = model_file.stem\n",
    "            print(f\"\\nüß™ Testing {model_name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Load model\n",
    "                with open(model_file, 'rb') as f:\n",
    "                    model = pickle.load(f)\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test_transformed)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                r2 = r2_score(y_true, y_pred)\n",
    "                mae = mean_absolute_error(y_true, y_pred)\n",
    "                rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "                \n",
    "                # Check for problems\n",
    "                if pd.isna(r2) or np.isinf(r2) or abs(r2) > 1e10:\n",
    "                    print(f\"  ‚ö†Ô∏è  R¬≤ problematic: {r2}\")\n",
    "                else:\n",
    "                    print(f\"  ‚úÖ R¬≤: {r2:.4f}\")\n",
    "                \n",
    "                if pd.isna(mae) or np.isinf(mae) or abs(mae) > 1e10:\n",
    "                    print(f\"  ‚ö†Ô∏è  MAE problematic: {mae}\")\n",
    "                else:\n",
    "                    print(f\"  ‚úÖ MAE: {mae:.4f}\")\n",
    "                \n",
    "                if pd.isna(rmse) or np.isinf(rmse) or abs(rmse) > 1e10:\n",
    "                    print(f\"  ‚ö†Ô∏è  RMSE problematic: {rmse}\")\n",
    "                else:\n",
    "                    print(f\"  ‚úÖ RMSE: {rmse:.4f}\")\n",
    "                \n",
    "                # Store if metrics are reasonable\n",
    "                if not (pd.isna(r2) or np.isinf(r2) or abs(r2) > 1e10):\n",
    "                    recalculated_metrics[model_name] = {\n",
    "                        'r2': float(r2),\n",
    "                        'mae': float(mae),\n",
    "                        'rmse': float(rmse)\n",
    "                    }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error testing {model_name}: {e}\")\n",
    "        \n",
    "        # Display recalculated metrics\n",
    "        if recalculated_metrics:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"RECALCULATED METRICS (VALID MODELS)\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            recalc_df = pd.DataFrame(recalculated_metrics).T\n",
    "            recalc_df = recalc_df.sort_values('r2', ascending=False)\n",
    "            display(recalc_df)\n",
    "            \n",
    "            # Save recalculated metrics\n",
    "            recalc_path = MODELS_PATH / \"recalculated_metrics.json\"\n",
    "            with open(recalc_path, 'w') as f:\n",
    "                json.dump(recalculated_metrics, f, indent=2)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Recalculated metrics saved to: {recalc_path}\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå No models produced valid metrics\")\n",
    "    else:\n",
    "        print(\"‚ùå Required columns not found\")\n",
    "else:\n",
    "    print(\"‚ùå Required files not found for recalculation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49241f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 12. Next Steps\n",
    "\n",
    "# %%\n",
    "print(\"=\"*60)\n",
    "print(\"NEXT STEPS & ACTION PLAN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüö® **CRITICAL ISSUES IDENTIFIED**:\")\n",
    "print(\"1. Models are producing infinite/NaN metrics\")\n",
    "print(\"2. This indicates serious problems in data, training, or evaluation\")\n",
    "print(\"3. The current results cannot be used for business decisions\")\n",
    "\n",
    "print(\"\\nüîß **IMMEDIATE ACTIONS REQUIRED**:\")\n",
    "print(\"\\n1. **Fix Data Issues**:\")\n",
    "print(\"   - Check the data_preparation.py script\")\n",
    "print(\"   - Verify Log_TotalClaims calculation\")\n",
    "print(\"   - Remove extreme outliers\")\n",
    "print(\"   - Code to run: python src/data_preparation.py --debug\")\n",
    "\n",
    "print(\"\\n2. **Retrain Models**:\")\n",
    "print(\"   - Start with LinearRegression as baseline\")\n",
    "print(\"   - Add proper regularization\")\n",
    "print(\"   - Use smaller dataset for debugging\")\n",
    "print(\"   - Code to run: python src/modelling/main.py --simple\")\n",
    "\n",
    "print(\"\\n3. **Debug Step-by-Step**:\")\n",
    "print(\"   - Train one model at a time\")\n",
    "print(\"   - Print intermediate predictions\")\n",
    "print(\"   - Verify each calculation\")\n",
    "\n",
    "print(\"\\nüìä **VALIDATION CHECKLIST**:\")\n",
    "print(\"‚úÖ Data has no NaN/Inf values\")\n",
    "print(\"‚úÖ Target variable is properly transformed\")\n",
    "print(\"‚úÖ Features are properly scaled\")\n",
    "print(\"‚úÖ Models converge during training\")\n",
    "print(\"‚úÖ Predictions are reasonable (not extreme)\")\n",
    "print(\"‚úÖ Metrics are calculated correctly\")\n",
    "\n",
    "print(\"\\nüí° **TROUBLESHOOTING TIPS**:\")\n",
    "print(\"‚Ä¢ Start with a small sample of data (1000 rows)\")\n",
    "print(\"‚Ä¢ Use simple LinearRegression first\")\n",
    "print(\"‚Ä¢ Print shapes and values at each step\")\n",
    "print(\"‚Ä¢ Compare predictions with actual values\")\n",
    "print(\"‚Ä¢ Check for data leakage\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"END OF DIAGNOSTIC ANALYSIS\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
