"""
Main orchestrator for the complete modeling pipeline.
"""
import sys
from pathlib import Path
import pandas as pd
import numpy as np
import logging
from datetime import datetime
import matplotlib.pyplot as plt
import pickle

from modeling import ClaimSeverityModeler
from featuring import FeatureEngineer
from data_preparation import ClaimDataPreparer
from extract_claim import ClaimDataExtractor

# Add the parent directory to path
sys.path.append(str(Path(__file__).parent))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('modeling_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ModelingPipeline:
    """Complete modeling pipeline with SHAP analysis."""

    def __init__(self, base_path: str = None):
        """Initialize the pipeline."""
        if base_path:
            self.base_path = Path(base_path)
        else:
            self.base_path = Path(__file__).parent.parent.parent

        # Define paths
        self.raw_data_path = self.base_path / "data" / \
            "raw" / "MachineLearningRating_v3.txt"
        self.processed_dir = self.base_path / "data" / "processed"
        self.models_dir = self.base_path / "models"
        self.results_dir = self.base_path / "results"
        self.reports_dir = self.results_dir / "task4_reports"

        # Create directories
        for dir_path in [self.processed_dir, self.models_dir,
                         self.results_dir, self.reports_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

        # Initialize components
        self.extractor = None
        self.preparer = None
        self.feature_engineer = None
        self.modeler = None

        logger.info(f"Base path: {self.base_path}")

    def run_complete_pipeline(self):
        """Run the complete modeling pipeline."""
        start_time = datetime.now()

        print("\n" + "="*80)
        print("CLAIM SEVERITY MODELING PIPELINE")
        print("="*80)

        try:
            # STEP 1: Extract claim data (skip if already exists)
            df_claims = self._step1_extract_data()

            # STEP 2: Prepare data
            X_train, X_test, y_train, y_test = self._step2_prepare_data(
                df_claims)

            # STEP 3: Feature engineering
            X_train_transformed, X_test_transformed = self._step3_feature_engineering(
                X_train, X_test, y_train
            )

            # STEP 4: Modeling
            results = self._step4_modeling(
                X_train_transformed, X_test_transformed, y_train, y_test
            )

            # STEP 5: SHAP Analysis (optional)
            self._step5_shap_analysis()

            # STEP 6: Generate comprehensive report
            self._step6_generate_report(start_time)

            runtime = datetime.now() - start_time

            print("\n" + "="*80)
            print("üéâ PIPELINE COMPLETED SUCCESSFULLY!")
            print("="*80)
            print(f"Total runtime: {runtime}")
            self._print_final_summary()

        except Exception as e:
            logger.error(f"Pipeline failed: {str(e)}", exc_info=True)
            print(f"\n‚ùå Pipeline failed with error: {str(e)}")
            raise

    def _step1_extract_data(self) -> pd.DataFrame:
        """Step 1: Extract claim data."""
        logger.info("\nüìä STEP 1: EXTRACTING CLAIM DATA")
        logger.info("-"*40)

        claim_file = self.processed_dir / 'claim_policies.csv'

        if claim_file.exists():
            logger.info(f"Loading existing claim data from: {claim_file}")
            df_claims = pd.read_csv(claim_file)
        else:
            logger.info("Extracting claim data from raw file...")
            self.extractor = ClaimDataExtractor(
                raw_data_path=str(self.raw_data_path),
                output_dir=str(self.processed_dir)
            )
            df_claims = self.extractor.extract_claim_records()

        logger.info(f"Loaded {len(df_claims)} claim records")
        return df_claims

    def _step2_prepare_data(self, df_claims: pd.DataFrame):
        """Step 2: Prepare data for modeling."""
        logger.info("\nüîß STEP 2: PREPARING DATA")
        logger.info("-"*40)

        self.preparer = ClaimDataPreparer(
            df_claims=df_claims,
            target_col='TotalClaims',
            test_size=0.2,
            random_state=42
        )

        df_prepared = self.preparer.prepare_data(
            save_path=str(self.processed_dir / 'claim_data_prepared.csv')
        )

        # Get training and testing data
        X_train, y_train = self.preparer.get_training_data()
        X_test, y_test = self.preparer.get_testing_data()

        logger.info(f"\nData prepared:")
        logger.info(f"  Training samples: {X_train.shape[0]}")
        logger.info(f"  Test samples: {X_test.shape[0]}")
        logger.info(f"  Features: {X_train.shape[1]}")

        return X_train, X_test, y_train, y_test

    def _step3_feature_engineering(self, X_train, X_test, y_train):
        """Step 3: Feature engineering and preprocessing."""
        logger.info("\n‚öôÔ∏è STEP 3: FEATURE ENGINEERING")
        logger.info("-"*40)

        self.feature_engineer = FeatureEngineer()

        # Create additional features WITHOUT target leakage
        logger.info("Creating additional features...")
        X_train_enhanced = self.feature_engineer.create_additional_features(
            X_train, is_training=True
        )
        X_test_enhanced = self.feature_engineer.create_additional_features(
            X_test, is_training=False
        )

        # Select features using correlation (for feature selection only, not creation)
        logger.info("Selecting features using correlation...")
        X_train_selected = self.feature_engineer.select_features_using_correlation(
            X_train_enhanced, y_train, threshold=0.05
        )

        # For test data, we need to align columns to match training selection
        # Get the columns selected from training
        selected_columns = X_train_selected.columns.tolist()

        # Ensure test data has the same columns (add missing, drop extra)
        X_test_selected = X_test_enhanced.copy()

        # Add missing columns with default values
        missing_cols = set(selected_columns) - set(X_test_selected.columns)
        for col in missing_cols:
            if col in self.feature_engineer.numerical_features:
                X_test_selected[col] = 0
            elif col in self.feature_engineer.categorical_features:
                X_test_selected[col] = 'missing'
            else:
                X_test_selected[col] = 0

        # Drop extra columns
        extra_cols = set(X_test_selected.columns) - set(selected_columns)
        if extra_cols:
            X_test_selected = X_test_selected.drop(columns=list(extra_cols))

        # Ensure column order matches
        X_test_selected = X_test_selected[selected_columns]

        # Select features using variance
        logger.info("Selecting features using variance...")
        X_train_selected = self.feature_engineer.select_features_using_variance(
            X_train_selected, threshold=0.01
        )

        # Update selected columns after variance selection
        selected_columns = X_train_selected.columns.tolist()

        # Align test data again
        X_test_selected = X_test_selected[selected_columns]

        # Fit and transform features
        logger.info("Transforming features...")
        X_train_transformed, X_test_transformed = self.feature_engineer.fit_transform_features(
            X_train_selected, X_test_selected, y_train
        )

        # Save preprocessor
        self.feature_engineer.save_preprocessor(
            str(self.models_dir / 'preprocessor.pkl'))

        logger.info(f"\nFeature engineering complete:")
        logger.info(f"  Original features: {X_train.shape[1]}")
        logger.info(f"  After engineering: {X_train_selected.shape[1]}")
        logger.info(f"  After transformation: {X_train_transformed.shape[1]}")

        return X_train_transformed, X_test_transformed

    def _step4_modeling(self, X_train, X_test, y_train, y_test):
        """Step 4: Model building and evaluation."""
        logger.info("\nü§ñ STEP 4: MODELING")
        logger.info("-"*40)

        self.modeler = ClaimSeverityModeler(model_dir=str(self.models_dir))

        # IMPORTANT: Apply log transformation to target for regression
        # This is critical for claim severity prediction
        logger.info("Applying log transformation to target variable...")
        y_train_log = np.log1p(y_train)
        y_test_log = np.log1p(y_test)

        # Train multiple models with LOG-transformed target
        models = self.modeler.train_models(
            X_train, y_train_log,  # Use log-transformed target
            X_test, y_test_log,
            use_cv=True,
            cv_folds=5
        )

        # Generate feature importance plot if feature names are available
        if (self.feature_engineer and
            hasattr(self.feature_engineer, 'feature_names') and
                self.feature_engineer.feature_names):

            logger.info("\nGenerating feature importance plot...")
            self.modeler.generate_feature_importance_plot(
                feature_names=self.feature_engineer.feature_names,
                top_n=20
            )

        # Hyperparameter tuning on best model
        if self.modeler.best_model_name:
            logger.info(
                f"\nüéØ Hyperparameter tuning for {self.modeler.best_model_name}")
            self.modeler.hyperparameter_tuning(
                X_train, y_train_log,  # Use log-transformed target
                model_name=self.modeler.best_model_name,
                cv_folds=3
            )

        # Generate comparison report
        comparison_df = self.modeler.generate_model_comparison_report(
            feature_names=self.feature_engineer.feature_names if self.feature_engineer else None
        )
        self.modeler.plot_model_comparison()

        # Save the best model
        if self.modeler.best_model:
            self.modeler.save_model(
                self.modeler.best_model, f'best_{self.modeler.best_model_name}')

        return self.modeler.results

    def _step5_shap_analysis(self):
        """Step 5: SHAP analysis for model interpretability."""
        logger.info("\nüîç STEP 5: SHAP ANALYSIS")
        logger.info("-"*40)

        # Check if all required components are available
        if not self.modeler or not self.modeler.best_model:
            logger.warning("No model available for SHAP analysis")
            return

        if not self.feature_engineer or not hasattr(self.feature_engineer, 'feature_names'):
            logger.warning("Feature names not available for SHAP analysis")
            return

        try:
            import shap

            # Get the transformed test data
            if hasattr(self.feature_engineer, 'X_test_transformed'):
                X_test_transformed = self.feature_engineer.X_test_transformed
            else:
                # Try to reconstruct test data
                test_file = self.processed_dir / 'test_data.csv'
                if test_file.exists():
                    test_df = pd.read_csv(test_file)
                    # Transform using the saved preprocessor
                    X_test_data = test_df.drop(
                        columns=['TotalClaims',
                                 'Log_TotalClaims', 'HighClaim'],
                        errors='ignore'
                    )
                    X_test_transformed = self.feature_engineer.transform_new_data(
                        X_test_data)
                else:
                    logger.warning("Test data not available for SHAP analysis")
                    return

            if X_test_transformed is not None:
                # Use a subset for SHAP (faster)
                X_test_subset = X_test_transformed[:30]

                # Choose explainer based on model type
                model_type = self.modeler.best_model_name

                if model_type in ['XGBoost', 'RandomForest', 'GradientBoosting', 'DecisionTree']:
                    try:
                        explainer = shap.TreeExplainer(self.modeler.best_model)
                        shap_values = explainer.shap_values(X_test_subset)
                    except Exception as e:
                        logger.warning(
                            f"TreeExplainer failed: {str(e)}. Using KernelExplainer instead.")
                        # Fall back to KernelExplainer
                        explainer = shap.KernelExplainer(
                            self.modeler.best_model.predict,
                            X_test_subset[:5]  # Use small background
                        )
                        shap_values = explainer.shap_values(X_test_subset)
                else:
                    # For linear models, use LinearExplainer
                    try:
                        explainer = shap.LinearExplainer(
                            self.modeler.best_model, X_test_subset)
                        shap_values = explainer.shap_values(X_test_subset)
                    except:
                        logger.warning("Skipping SHAP for this model type")
                        return

                # Create SHAP directory
                shap_dir = self.reports_dir / 'SHAP_Analysis'
                shap_dir.mkdir(exist_ok=True)

                # Get feature names
                feature_names = self.feature_engineer.feature_names
                if len(feature_names) > X_test_subset.shape[1]:
                    feature_names = feature_names[:X_test_subset.shape[1]]

                # 1. Summary plot
                plt.figure(figsize=(12, 8))
                if hasattr(shap_values, 'shape') and len(shap_values.shape) > 1:
                    shap.summary_plot(shap_values, X_test_subset,
                                      feature_names=feature_names,
                                      show=False)
                    plt.title(
                        f'SHAP Summary Plot - {self.modeler.best_model_name}', fontsize=16)
                    plt.tight_layout()
                    plt.savefig(shap_dir / 'shap_summary.png',
                                dpi=300, bbox_inches='tight')
                    plt.close()

                    # 2. Bar plot
                    plt.figure(figsize=(10, 6))
                    shap.summary_plot(shap_values, X_test_subset,
                                      plot_type="bar",
                                      feature_names=feature_names,
                                      show=False)
                    plt.title(
                        f'SHAP Feature Importance - {self.modeler.best_model_name}', fontsize=16)
                    plt.tight_layout()
                    plt.savefig(shap_dir / 'shap_bar.png',
                                dpi=300, bbox_inches='tight')
                    plt.close()

                    # 3. Calculate mean absolute SHAP values
                    if hasattr(shap_values, 'values'):
                        shap_values_array = shap_values.values
                    else:
                        shap_values_array = shap_values

                    mean_abs_shap = np.abs(shap_values_array).mean(axis=0)

                    # Create feature importance DataFrame
                    shap_importance = pd.DataFrame({
                        'feature': feature_names[:len(mean_abs_shap)],
                        'shap_importance': mean_abs_shap
                    }).sort_values('shap_importance', ascending=False)

                    # Save to CSV
                    shap_importance.to_csv(
                        shap_dir / 'shap_feature_importance.csv', index=False)

                    logger.info(f"‚úÖ SHAP analysis complete")
                    logger.info(f"Top 5 most influential features:")
                    for idx, row in shap_importance.head(5).iterrows():
                        logger.info(
                            f"  {row['feature']}: {row['shap_importance']:.4f}")

                    # Generate business insights
                    self._generate_shap_insights(shap_importance)

        except ImportError:
            logger.warning(
                "SHAP not installed. Install with: pip install shap")
        except Exception as e:
            logger.error(f"Error in SHAP analysis: {str(e)}")

    def _generate_shap_insights(self, shap_importance: pd.DataFrame):
        """Generate business insights from SHAP analysis."""
        if not self.modeler:
            return

        insights_file = self.reports_dir / 'business_insights.md'

        with open(insights_file, 'w') as f:
            f.write("# Business Insights from SHAP Analysis\n\n")
            f.write(f"**Model:** {self.modeler.best_model_name}\n")
            f.write(
                f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            f.write("## Top 10 Most Influential Features\n\n")
            f.write(
                "| Rank | Feature | SHAP Importance | Business Interpretation |\n")
            f.write(
                "|------|---------|-----------------|-------------------------|\n")

            # Define interpretations for common features
            interpretations = {
                'log_SumInsured': "Higher insured values lead to higher predicted claims",
                'log_TotalPremium': "Higher premiums indicate higher risk profiles",
                'VehicleAge': "Older vehicles have higher predicted claim amounts",
                'SumInsured': "Direct relationship with claim amounts",
                'TotalPremium': "Premium amount reflects risk assessment",
                'PremiumToSumInsuredRatio': "Higher ratio indicates perceived higher risk",
                'HasHighPower': "High power vehicles have higher claim predictions",
                'IsOldVehicle': "Old vehicles (>10 years) have higher predicted claims",
                'Age_Value_Interaction': "Interaction between age and value affects claims",
                'Risk_Score': "Composite risk score influences predictions",
                'cubiccapacity': "Larger engine capacity associated with higher claims",
                'kilowatts': "Higher power output increases claim predictions"
            }

            for idx, row in shap_importance.head(10).iterrows():
                feature = row['feature']
                importance = row['shap_importance']

                # Get interpretation
                interpretation = interpretations.get(
                    feature, "Significant impact on claim predictions")

                # Clean feature name for display
                display_feature = feature.replace('_', ' ').title()

                f.write(
                    f"| {idx+1} | {display_feature} | {importance:.4f} | {interpretation} |\n")

            f.write("\n## Key Business Recommendations\n\n")
            f.write(
                "1. **Risk-Based Pricing**: Use the model to implement dynamic pricing based on vehicle characteristics\n")
            f.write(
                "2. **Underwriting Guidelines**: Focus on high-risk features identified by SHAP analysis\n")
            f.write(
                "3. **Loss Prevention**: Target interventions for vehicles with high-risk characteristics\n")
            f.write(
                "4. **Portfolio Optimization**: Adjust portfolio mix based on risk factors identified\n")
            f.write(
                "5. **Product Design**: Create specialized products for different risk segments\n")
            f.write(
                "6. **Fraud Detection**: Monitor claims that deviate significantly from model predictions\n")
            f.write(
                "7. **Customer Segmentation**: Group customers by risk profile for targeted marketing\n")

        logger.info(f"üìä Business insights saved to: {insights_file}")

    def _step6_generate_report(self, start_time: datetime):
        """Step 6: Generate comprehensive report."""
        logger.info("\nüìä STEP 6: GENERATING REPORTS")
        logger.info("-"*40)

        runtime = datetime.now() - start_time

        # Create comprehensive report
        report = {
            "project": "Task 4 - Claim Severity Prediction",
            "timestamp": datetime.now().isoformat(),
            "runtime_seconds": runtime.total_seconds(),
            "best_model": {
                "name": self.modeler.best_model_name if self.modeler else None,
                "performance": self.modeler.results.get(self.modeler.best_model_name, {}) if self.modeler and self.modeler.best_model_name else {}
            },
            "all_models": self.modeler.results if self.modeler else {},
            "dataset_info": {
                "training_samples": self.preparer.X_train.shape[0] if self.preparer and hasattr(self.preparer, 'X_train') else None,
                "test_samples": self.preparer.X_test.shape[0] if self.preparer and hasattr(self.preparer, 'X_test') else None,
                "features_count": self.preparer.X_train.shape[1] if self.preparer and hasattr(self.preparer, 'X_train') else None
            },
            "feature_engineering": {
                "numerical_features": len(self.feature_engineer.numerical_features) if self.feature_engineer and self.feature_engineer.numerical_features else 0,
                "categorical_features": len(self.feature_engineer.categorical_features) if self.feature_engineer and self.feature_engineer.categorical_features else 0,
                "total_features_after_engineering": len(self.feature_engineer.feature_names) if self.feature_engineer and hasattr(self.feature_engineer, 'feature_names') else 0
            }
        }

        # Save JSON report
        report_file = self.reports_dir / 'task4_comprehensive_report.json'
        with open(report_file, 'w') as f:
            import json
            json.dump(report, f, indent=2)

        # Generate markdown report
        self._generate_markdown_report(report)

        logger.info(f"üìä Reports saved to: {self.reports_dir}")

    def _generate_markdown_report(self, report: dict):
        """Generate markdown format report."""
        md_file = self.reports_dir / 'task4_final_report.md'

        with open(md_file, 'w') as f:
            f.write("# Task 4: Claim Severity Prediction - Final Report\n\n")

            f.write("## Executive Summary\n\n")
            if report['best_model']['name']:
                best_metrics = report['best_model']['performance']
                f.write(
                    f"The claim severity prediction model was successfully built and evaluated. ")
                f.write(
                    f"The best-performing model was **{report['best_model']['name']}** ")
                if 'r2' in best_metrics:
                    f.write(
                        f"with an R¬≤ score of {best_metrics.get('r2', 0):.4f}.\n\n")
                else:
                    f.write("with detailed performance metrics below.\n\n")
            else:
                f.write("Model training completed. See detailed results below.\n\n")

            f.write("## Model Performance Comparison\n\n")
            if report['all_models']:
                f.write("| Model | R¬≤ Score | RMSE (R) | MAE (R) | MAPE |\n")
                f.write("|-------|----------|----------|---------|------|\n")

                for model_name, metrics in report['all_models'].items():
                    # Handle potentially large numbers
                    rmse = metrics.get('rmse', 0)
                    mae = metrics.get('mae', 0)

                    if rmse > 1e6:
                        rmse_str = f"{rmse:.2e}"
                    else:
                        rmse_str = f"{rmse:,.0f}"

                    if mae > 1e6:
                        mae_str = f"{mae:.2e}"
                    else:
                        mae_str = f"{mae:,.0f}"

                    f.write(f"| {model_name} | {metrics.get('r2', 0):.4f} | R{rmse_str} | "
                            f"R{mae_str} | {metrics.get('mape', 0):.1f}% |\n")
            else:
                f.write("No model results available.\n")
            f.write("\n")

            f.write("## Dataset Information\n\n")
            f.write(
                f"- **Training Samples:** {report['dataset_info']['training_samples']:,}\n")
            f.write(
                f"- **Test Samples:** {report['dataset_info']['test_samples']:,}\n")
            f.write(
                f"- **Features:** {report['dataset_info']['features_count']}\n")
            f.write(
                f"- **Numerical Features:** {report['feature_engineering']['numerical_features']}\n")
            f.write(
                f"- **Categorical Features:** {report['feature_engineering']['categorical_features']}\n")
            f.write(
                f"- **Total Features After Engineering:** {report['feature_engineering']['total_features_after_engineering']}\n\n")

            f.write("## Files Generated\n\n")
            f.write("### Models\n")
            if report['best_model']['name']:
                f.write(
                    f"- `best_{report['best_model']['name']}.pkl`: Best trained model\n")
            f.write("- `preprocessor.pkl`: Feature preprocessor\n")
            f.write("- `model_comparison.csv`: Model performance comparison\n")
            f.write("- `model_comparison.json`: Detailed model metrics\n")
            f.write(
                "- `feature_importance_*.png`: Feature importance visualizations\n")
            f.write("- `feature_importance_*.csv`: Feature importance data\n\n")

            f.write("### Visualizations\n")
            f.write(
                "- `model_comparison_*.png`: Model performance comparison plots\n")
            f.write("- `*_evaluation.png`: Individual model evaluation plots\n")
            f.write("- `shap_summary.png`: SHAP summary plot (if SHAP installed)\n")
            f.write("- `shap_bar.png`: SHAP feature importance plot\n\n")

            f.write("### Reports\n")
            f.write("- `task4_comprehensive_report.json`: Complete results\n")
            f.write("- `task4_final_report.md`: This report\n")
            f.write("- `business_insights.md`: Business insights from SHAP\n")
            f.write("- `shap_feature_importance.csv`: SHAP importance values\n")
            f.write("- `cross_validation_results.json`: Cross-validation results\n\n")

            f.write("## Next Steps\n\n")
            f.write(
                "1. **Model Deployment**: Integrate the best model into production systems\n")
            f.write(
                "2. **Monitoring**: Implement monitoring for model performance drift\n")
            f.write(
                "3. **Retraining**: Schedule regular model retraining with new data\n")
            f.write(
                "4. **A/B Testing**: Test the model's impact on pricing and profitability\n")
            f.write(
                "5. **Feature Store**: Create a feature store for consistent feature engineering\n")
            f.write(
                "6. **Model Explainability**: Expand SHAP analysis for business stakeholders\n")
            f.write(
                "7. **Automation**: Automate the entire pipeline for regular updates\n")

            # Evaluation Metrics Note
            f.write("## Note on Evaluation Metrics\n\n")
            f.write(
                "The project requirements mention classification metrics (accuracy, precision, recall, F1-score), ")
            f.write(
                "however Task 4 specifically requires **claim severity prediction**, which is a regression problem. ")
            f.write(
                "Therefore, we have implemented appropriate regression metrics:\n")
            f.write(
                "- **R¬≤ (R-squared)**: Proportion of variance explained by the model\n")
            f.write(
                "- **RMSE (Root Mean Squared Error)**: Average prediction error in Rand\n")
            f.write(
                "- **MAE (Mean Absolute Error)**: Average absolute prediction error\n")
            f.write(
                "- **MAPE (Mean Absolute Percentage Error)**: Average percentage error\n\n")
            f.write("These metrics are standard for regression problems and more appropriate for predicting continuous claim amounts.\n\n")

    def _print_final_summary(self):
        """Print final summary to console."""
        print("\n" + "="*80)
        print("FINAL RESULTS SUMMARY")
        print("="*80)

        if self.modeler and self.modeler.best_model_name:
            print(f"\nüèÜ BEST MODEL: {self.modeler.best_model_name}")
            if self.modeler.best_model_name in self.modeler.results:
                metrics = self.modeler.results[self.modeler.best_model_name]
                print(f"  R¬≤ Score: {metrics.get('r2', 'N/A')}")
                print(f"  RMSE: R{metrics.get('rmse', 'N/A'):,.2f}")
                print(f"  MAE: R{metrics.get('mae', 'N/A'):,.2f}")
                print(f"  MAPE: {metrics.get('mape', 'N/A'):.2f}%")
        else:
            print("\n‚ö†Ô∏è  No best model identified")

        print(f"\nüìÅ OUTPUT DIRECTORIES:")
        print(f"  Processed Data: {self.processed_dir}")
        print(f"  Models: {self.models_dir}")
        print(f"  Results: {self.results_dir}")
        print(f"  Reports: {self.reports_dir}")

        print(f"\nüìä KEY FILES GENERATED:")
        if self.modeler and self.modeler.best_model_name:
            print(
                f"  ‚Ä¢ best_{self.modeler.best_model_name}.pkl - Best trained model")
        print(f"  ‚Ä¢ claim_data_prepared.csv - Prepared dataset")
        print(f"  ‚Ä¢ model_comparison.csv - Model performance comparison")
        print(f"  ‚Ä¢ feature_importance_*.png - Feature importance visualizations")
        print(f"  ‚Ä¢ business_insights.md - Business recommendations")

        print(f"\n‚úÖ TASK 4 REQUIREMENTS COMPLETED:")
        print(f"  ‚úì Data preparation with feature engineering")
        print(f"  ‚úì Multiple model implementation (Linear, Random Forest, XGBoost, etc.)")
        print(f"  ‚úì Model evaluation with RMSE and R¬≤ metrics")
        print(f"  ‚úì Feature importance analysis")
        print(f"  ‚úì Comprehensive reporting")

        print("\n" + "="*80)


def main():
    """Main execution function."""
    # Initialize pipeline
    pipeline = ModelingPipeline()

    # Run complete pipeline
    pipeline.run_complete_pipeline()


if __name__ == "__main__":
    main()
